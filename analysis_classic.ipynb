{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f56a6caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# GRAPH SETTINGS\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32991b7b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d177eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_version = 'v3.1'\n",
    "map_nr = 'encouraged_division_of_labor'\n",
    "\n",
    "# Directorios base\n",
    "local = '/mnt/lustre/home/samuloza'\n",
    "#local = ''\n",
    "#local = 'C:/OneDrive - Universidad Complutense de Madrid (UCM)/Doctorado'\n",
    "\n",
    "if intent_version is not None:\n",
    "    raw_dir = f\"{local}/data/samuel_lozano/cooked/classic/{intent_version}/map_{map_nr}\"\n",
    "else:\n",
    "    raw_dir = f\"{local}/data/samuel_lozano/cooked/classic/map_{map_nr}\"\n",
    "\n",
    "base_dirs = {\n",
    "    \"Competitive\": f\"{raw_dir}/competitive\",\n",
    "    \"Cooperative\": f\"{raw_dir}/cooperative\"\n",
    "}\n",
    "\n",
    "# Crear directorios base si no existen\n",
    "for dir_path in base_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "output_path = f\"{raw_dir}/training_results.csv\"\n",
    "figures_dir = f\"{raw_dir}/figures/\"\n",
    "\n",
    "# Crear también el directorio para las figuras si no existe\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "# Eliminar el archivo CSV si ya existe\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4157bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# Patrón para capturar los coeficientes de recompensa\n",
    "reward_pattern = re.compile(\n",
    "    r\"'([^']+)':\\s*\\(\\s*([-\\d\\.eE+]+),\\s*([-\\d\\.eE+]+)\\)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6741c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_type, base_dir in base_dirs.items():\n",
    "    game_flag = 1 if \"Cooperative\" in game_type else 0\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        date_time_str = folder.replace(\"Training_\", \"\")\n",
    "        config_path = os.path.join(folder_path, \"config.txt\")\n",
    "        csv_path = os.path.join(folder_path, \"training_stats.csv\")\n",
    "\n",
    "        if not (os.path.exists(config_path) and os.path.exists(csv_path)):\n",
    "            continue\n",
    "\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_contents = f.read()\n",
    "        match = reward_pattern.findall(config_contents)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        (name_1, alpha_1, beta_1), (name_2, alpha_2, beta_2) = match\n",
    "        alpha_1, beta_1 = float(alpha_1), float(beta_1) \n",
    "        alpha_2, beta_2 = float(alpha_2), float(beta_2)\n",
    "\n",
    "        lr_match = re.search(r\"LR:\\s*([0-9.eE+-]+)\", config_contents)\n",
    "        lr = float(lr_match.group(1)) \n",
    "\n",
    "        with open(csv_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        header = lines[0]\n",
    "        filtered_lines = [header] + [line for line in lines[1:] if not line.startswith(\"episode,env\")]\n",
    "\n",
    "        from io import StringIO\n",
    "        df = pd.read_csv(StringIO(\"\".join(filtered_lines)))\n",
    "\n",
    "        df.iloc[:, 0] = range(1, len(df) + 1)\n",
    "\n",
    "        df.insert(0, \"timestamp\", date_time_str)\n",
    "        df.insert(1, \"game_type\", game_flag)\n",
    "        df.insert(2, \"alpha_1\", alpha_1)\n",
    "        df.insert(3, \"beta_1\", beta_1)\n",
    "        df.insert(4, \"alpha_2\", alpha_2)\n",
    "        df.insert(5, \"beta_2\", beta_2)\n",
    "        df.insert(6, \"lr\", lr)\n",
    "\n",
    "        all_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77888e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los resultados\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24023d0f",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0966578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el CSV especificando los tipos de datos\n",
    "dtype_dict = {\n",
    "    \"timestamp\": str,\n",
    "    \"game_type\": int,\n",
    "    \"alpha_1\": float,\n",
    "    \"beta_1\": float,\n",
    "    \"alpha_2\": float,\n",
    "    \"beta_2\": float\n",
    "}\n",
    "\n",
    "df = pd.read_csv(output_path, dtype=dtype_dict, low_memory=False)\n",
    "for col in df.columns[6:]:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Crear una columna identificadora de combinación de coeficientes\n",
    "df = df.sort_values(by=[\"alpha_1\", \"alpha_2\"], ascending=[False, False])\n",
    "df[\"attitude_key\"] = df.apply(lambda row: f\"{row['alpha_1']}_{row['beta_1']}_{row['alpha_2']}_{row['beta_2']}\", axis=1)\n",
    "df[\"pure_reward_total\"] = df[\"pure_reward_ai_rl_1\"] + df[\"pure_reward_ai_rl_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4aefc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar todas las combinaciones únicas\n",
    "unique_attitudes = df[\"attitude_key\"].unique()\n",
    "unique_lr = df[\"lr\"].unique()\n",
    "unique_game_type = df[\"game_type\"].unique()\n",
    "\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "rewarded_metrics_1 = [\n",
    "    \"delivered_ai_rl_1\",\n",
    "    \"cut_ai_rl_1\",\n",
    "    \"salad_ai_rl_1\",\n",
    "]\n",
    "\n",
    "rewarded_metrics_2 = [\n",
    "    \"delivered_ai_rl_2\",\n",
    "    \"cut_ai_rl_2\",\n",
    "    \"salad_ai_rl_2\",\n",
    "]\n",
    "\n",
    "rewarded_metric_labels = {\n",
    "    \"delivered\": \"Delivered\",\n",
    "    \"cut\": \"Cut\",\n",
    "    \"salad\": \"Salad\"\n",
    "}\n",
    "\n",
    "rewarded_metric_colors = {\n",
    "    \"delivered\": \"#27AE60\",\n",
    "    \"cut\": \"#2980B9\",\n",
    "    \"salad\": \"#E67E22\"\n",
    "}\n",
    "\n",
    "action_types_1 = [\n",
    "    \"do_nothing_ai_rl_1\",\n",
    "    \"floor_actions_ai_rl_1\",\n",
    "    \"wall_actions_ai_rl_1\",\n",
    "    \"useless_counter_actions_ai_rl_1\",\n",
    "    \"useful_counter_actions_ai_rl_1\",\n",
    "    \"useless_food_dispenser_actions_ai_rl_1\",\n",
    "    \"useful_food_dispenser_actions_ai_rl_1\",\n",
    "    \"useless_cutting_board_actions_ai_rl_1\",\n",
    "    \"useful_cutting_board_actions_ai_rl_1\",\n",
    "    \"useless_plate_dispenser_actions_ai_rl_1\",\n",
    "    \"useful_plate_dispenser_actions_ai_rl_1\",\n",
    "    \"useless_delivery_actions_ai_rl_1\",\n",
    "    \"useful_delivery_actions_ai_rl_1\",\n",
    "]\n",
    "\n",
    "action_types_2 = [\n",
    "    \"do_nothing_ai_rl_2\",\n",
    "    \"floor_actions_ai_rl_2\",\n",
    "    \"wall_actions_ai_rl_2\",\n",
    "    \"useless_counter_actions_ai_rl_2\",\n",
    "    \"useful_counter_actions_ai_rl_2\",\n",
    "    \"useless_food_dispenser_actions_ai_rl_2\",\n",
    "    \"useful_food_dispenser_actions_ai_rl_2\",\n",
    "    \"useless_cutting_board_actions_ai_rl_2\",\n",
    "    \"useful_cutting_board_actions_ai_rl_2\",\n",
    "    \"useless_plate_dispenser_actions_ai_rl_2\",\n",
    "    \"useful_plate_dispenser_actions_ai_rl_2\",\n",
    "    \"useless_delivery_actions_ai_rl_2\",\n",
    "    \"useful_delivery_actions_ai_rl_2\",\n",
    "]\n",
    "\n",
    "action_types_labels = {\n",
    "    \"do_nothing\": \"No action\",\n",
    "    \"floor_actions\": \"Floor\",\n",
    "    \"wall_actions\": \"Wall\",\n",
    "    \"useless_counter_actions\": \"Useless Counter\",\n",
    "    \"useful_counter_actions\": \"Useful Counter\",\n",
    "    \"useless_food_dispenser_actions\": \"Useless Food Dispenser\",\n",
    "    \"useful_food_dispenser_actions\": \"Useful Food Dispenser\",\n",
    "    \"useless_cutting_board_actions\": \"Useless Cutting Board\",\n",
    "    \"useful_cutting_board_actions\": \"Useful Cutting Board\",\n",
    "    \"useless_plate_dispenser_actions\": \"Useless Plate Dispenser\",\n",
    "    \"useful_plate_dispenser_actions\": \"Useful Plate Dispenser\",\n",
    "    \"useless_delivery_actions\": \"Useless Delivery\",\n",
    "    \"useful_delivery_actions\": \"Useful Delivery\",\n",
    "}\n",
    "\n",
    "action_types_colors = {\n",
    "    \"do_nothing\": \"#000000\",  # negro\n",
    "    # floor & wall (colores random porque nunca aparecen)\n",
    "    \"floor_actions\": \"#9B59B6\",  # morado random\n",
    "    \"wall_actions\": \"#59351F\",   # marrón random\n",
    "    # Counter (gris)\n",
    "    \"useless_counter_actions\": \"#D5D8DC\",  # gris claro\n",
    "    \"useful_counter_actions\": \"#7B7D7D\",   # gris oscuro\n",
    "    # Food dispenser (rojo)\n",
    "    \"useless_food_dispenser_actions\": \"#F5B7B1\",  # rojo claro\n",
    "    \"useful_food_dispenser_actions\": \"#C0392B\",   # rojo oscuro\n",
    "    # Cutting board (azul)\n",
    "    \"useless_cutting_board_actions\": \"#AED6F1\",  # azul claro\n",
    "    \"useful_cutting_board_actions\": \"#2980B9\",   # azul oscuro\n",
    "    # Plate dispenser (naranja)\n",
    "    \"useless_plate_dispenser_actions\": \"#FAD7A0\",  # naranja claro\n",
    "    \"useful_plate_dispenser_actions\": \"#E67E22\",   # naranja oscuro\n",
    "    # Delivery (verde)\n",
    "    \"useless_delivery_actions\": \"#A9DFBF\",  # verde claro\n",
    "    \"useful_delivery_actions\": \"#27AE60\",  # verde normal\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb9a24",
   "metadata": {},
   "source": [
    "## General visuals (not smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7361c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Pure total reward vs epoch\n",
    "\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for game_type in unique_game_type:\n",
    "        game_type_filtered = subset[subset[\"game_type\"] == game_type]\n",
    "    \n",
    "        # Filtrar por tasa de aprendizaje\n",
    "        for lr in unique_lr:\n",
    "            lr_filtered = game_type_filtered[game_type_filtered[\"lr\"] == lr]\n",
    "            grouped = lr_filtered.groupby(\"epoch\")[\"pure_reward_total\"].mean().reset_index()\n",
    "            label = f\"Game Type {game_type}, LR {lr}\"\n",
    "            plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_total\"], label=label)\n",
    "    \n",
    "    # Añadir detalles\n",
    "    plt.title(f\"Pure Reward vs Epoch\\nAttitude {attitude}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    #plt.xlim([0,100])\n",
    "    plt.ylabel(\"Pure Reward Total\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    sanitized_attitude = attitude.replace('.', 'p')\n",
    "    filename = f\"pure_reward_attitude_{sanitized_attitude}.png\"\n",
    "    filepath = os.path.join(figures_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "479bc358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Each agent pure total reward vs epoch\n",
    "\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    for game_type in unique_game_type:\n",
    "        game_type_filtered = subset[subset[\"game_type\"] == game_type]\n",
    "    \n",
    "        # Crear la figura\n",
    "        plt.figure(figsize=(10, 6))\n",
    "    \n",
    "        # Filtrar por tasa de aprendizaje\n",
    "        for lr in unique_lr:    \n",
    "            lr_filtered = game_type_filtered[game_type_filtered[\"lr\"] == lr]\n",
    "            grouped = lr_filtered.groupby(\"epoch\")[[\"pure_reward_ai_rl_1\", \"pure_reward_ai_rl_2\"]].mean().reset_index()\n",
    "            label_0 = f\"Agent 1, LR {lr}\"\n",
    "            label_1 = f\"Agent 2, LR {lr}\"\n",
    "            plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_ai_rl_1\"], label=label_0)\n",
    "            plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_ai_rl_2\"], label=label_1)\n",
    "    \n",
    "        # Añadir detalles\n",
    "        plt.title(f\"Pure Reward vs Epoch\\nAttitude {attitude}, Game Type {game_type}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        #plt.xlim([0,100])\n",
    "        plt.ylabel(\"Pure Reward Total\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        sanitized_attitude = attitude.replace('.', 'p')\n",
    "        filename = f\"pure_reward_agents_g{game_type}_attitude_{sanitized_attitude}.png\"\n",
    "        filepath = os.path.join(figures_dir, filename)\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab6773c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print agent metrics vs epoch\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    att_parts = attitude.split('_')\n",
    "    att0_title = f\"{att_parts[0]}_{att_parts[1]}\"\n",
    "    att1_title = f\"{att_parts[2]}_{att_parts[3]}\"\n",
    "\n",
    "    for game_type in [0, 1]:\n",
    "        for lr in subset[\"lr\"].unique():\n",
    "            filtered_subset = subset[(subset[\"game_type\"] == game_type) & (subset[\"lr\"] == lr)]\n",
    "    \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for metric in rewarded_metrics_1:\n",
    "                grouped = filtered_subset.groupby([\"epoch\"])[metric].mean().reset_index()\n",
    "                plt.plot(grouped[\"epoch\"], grouped[metric], label=metric.replace(\"_\", \" \").title())\n",
    "            plt.title(f\"Metrics per Epoch - Game Type {game_type}, LR {lr}, Attitude {att0_title}\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Mean value\")\n",
    "            plt.legend()\n",
    "            #plt.xlim([0, 100])\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            sanitized_attitude = attitude.replace('.', 'p')\n",
    "            filename_1 = f\"rewarded_metrics_agent1_g{game_type}_lr{str(lr).replace('.', 'p')}_attitude_{sanitized_attitude}.png\"\n",
    "            filepath_1 = os.path.join(figures_dir, filename_1)\n",
    "            plt.savefig(filepath_1)\n",
    "            plt.close()\n",
    "    \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for metric in rewarded_metrics_2:\n",
    "                grouped = filtered_subset.groupby([\"epoch\"])[metric].mean().reset_index()\n",
    "                plt.plot(grouped[\"epoch\"], grouped[metric], label=metric.replace(\"_\", \" \").title())\n",
    "            plt.title(f\"Metrics per Epoch - Game Type {game_type}, LR {lr}, Attitude {att1_title}\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Mean value\")\n",
    "            plt.legend()\n",
    "            #plt.xlim([0, 100])\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename_2 = f\"rewarded_metrics_agent2_g{game_type}_lr{str(lr).replace('.', 'p')}_attitude_{sanitized_attitude}.png\"\n",
    "            filepath_2 = os.path.join(figures_dir, filename_2)\n",
    "            plt.savefig(filepath_2)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d275b59",
   "metadata": {},
   "source": [
    "# Smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3c811ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "\n",
    "smoothed_figures_dir = f\"{figures_dir}/smoothed_{N}/\"\n",
    "os.makedirs(smoothed_figures_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e132a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Pure total reward vs epoch\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for game_type in unique_game_type:\n",
    "    game_type_filtered_df = df[df[\"game_type\"] == game_type]\n",
    "\n",
    "    # Filtrar por tasa de aprendizaje\n",
    "    for lr in unique_lr:\n",
    "        lr_filtered_df = game_type_filtered_df[game_type_filtered_df[\"lr\"] == lr]\n",
    "        lr_filtered_df[\"epoch_block\"] = (lr_filtered_df[\"epoch\"] // N)\n",
    "        block_means = lr_filtered_df.groupby(\"epoch_block\")[\"pure_reward_total\"].mean()\n",
    "        middle_epochs = lr_filtered_df.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "\n",
    "        if game_type == 0:\n",
    "            label = f\"Competitive\"\n",
    "            color = \"red\"\n",
    "        else:\n",
    "            label = f\"Cooperative\"\n",
    "            color = \"green\"\n",
    "        \n",
    "        plt.plot(middle_epochs, block_means, label=label, color=color)\n",
    "\n",
    "# Añadir detalles\n",
    "plt.xlabel(\"Epochs\", fontsize=20)\n",
    "#plt.xlim([0,100])\n",
    "plt.ylabel(\"Mean total reward\", fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = f\"pure_reward_smoothed_{N}.png\"\n",
    "filepath = os.path.join(smoothed_figures_dir, filename)\n",
    "plt.savefig(filepath)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03aae776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Each agent pure total reward vs epoch\n",
    "\n",
    "for game_type in unique_game_type:\n",
    "    game_type_filtered = df[df[\"game_type\"] == game_type]\n",
    "\n",
    "    # Crear la figura\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Filtrar por tasa de aprendizaje\n",
    "    for lr in unique_lr:\n",
    "        lr_filtered = game_type_filtered[game_type_filtered[\"lr\"] == lr]\n",
    "        lr_filtered[\"epoch_block\"] = (lr_filtered[\"epoch\"] // N)\n",
    "        block_means_1 = lr_filtered.groupby(\"epoch_block\")[\"pure_reward_ai_rl_1\"].mean()\n",
    "        block_means_2 = lr_filtered.groupby(\"epoch_block\")[\"pure_reward_ai_rl_2\"].mean()\n",
    "        middle_epochs = lr_filtered.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "\n",
    "        label_1 = f\"Agent 1, LR {lr}\"\n",
    "        label_2 = f\"Agent 2, LR {lr}\"\n",
    "        plt.plot(middle_epochs, block_means_1, label=label_1)\n",
    "        plt.plot(middle_epochs, block_means_2, label=label_2)\n",
    "\n",
    "    # Añadir detalles\n",
    "    plt.title(f\"Pure Reward vs Epoch\\nGame Type {game_type}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    #plt.xlim([0,100])\n",
    "    plt.ylabel(\"Pure Reward Total\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"pure_reward_agents_g{game_type}_smoothed_{N}.png\"\n",
    "    filepath = os.path.join(smoothed_figures_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e72af4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Each agent modified total reward vs epoch\n",
    "\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    for game_type in unique_game_type:\n",
    "        game_type_filtered = subset[subset[\"game_type\"] == game_type]\n",
    "    \n",
    "        # Crear la figura\n",
    "        plt.figure(figsize=(10, 6))\n",
    "    \n",
    "        # Filtrar por tasa de aprendizaje\n",
    "        for lr in unique_lr:\n",
    "            lr_filtered = game_type_filtered[game_type_filtered[\"lr\"] == lr]\n",
    "            lr_filtered[\"epoch_block\"] = (lr_filtered[\"epoch\"] // N)\n",
    "            block_means_1 = lr_filtered.groupby(\"epoch_block\")[\"modified_reward_ai_rl_1\"].mean()\n",
    "            block_means_2 = lr_filtered.groupby(\"epoch_block\")[\"modified_reward_ai_rl_2\"].mean()\n",
    "            middle_epochs = lr_filtered.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "\n",
    "            label_1 = f\"Agent 1, LR {lr}\"\n",
    "            label_2 = f\"Agent 2, LR {lr}\"\n",
    "            plt.plot(middle_epochs, block_means_1, label=label_1)\n",
    "            plt.plot(middle_epochs, block_means_2, label=label_2)\n",
    "    \n",
    "        # Añadir detalles\n",
    "        plt.title(f\"Modified Reward vs Epoch\\nAttitude {attitude}, Game Type {game_type}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        #plt.xlim([0,100])\n",
    "        plt.ylabel(\"Modified Reward Total\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        sanitized_attitude = attitude.replace('.', 'p')\n",
    "        filename = f\"modified_reward_agents_g{game_type}_attitude_{sanitized_attitude}_smoothed_{N}.png\"\n",
    "        filepath = os.path.join(smoothed_figures_dir, filename)\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "182370db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print agent rewarded metrics vs epoch\n",
    "\n",
    "for game_type in unique_game_type:\n",
    "    for lr in unique_lr:\n",
    "        filtered_subset = df[(df[\"game_type\"] == game_type) & (df[\"lr\"] == lr)]\n",
    "        filtered_subset[\"epoch_block\"] = (filtered_subset[\"epoch\"] // N)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for metric in rewarded_metrics_1:\n",
    "            block_means = filtered_subset.groupby(\"epoch_block\")[[metric]].mean()\n",
    "            middle_epochs = filtered_subset.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "            plt.plot(middle_epochs, block_means, label=metric.replace(\"_\", \" \").title())\n",
    "        plt.title(f\"Metrics per Epoch - Game Type {game_type}, LR {lr}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Mean value\")\n",
    "        plt.legend()\n",
    "        #plt.xlim([0, 100])\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename_1 = f\"rewarded_metrics_agent1_g{game_type}_lr{str(lr).replace('.', 'p')}_smoothed_{N}.png\"\n",
    "        filepath_1 = os.path.join(smoothed_figures_dir, filename_1)\n",
    "        plt.savefig(filepath_1)\n",
    "        plt.close()\n",
    "    \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for metric in rewarded_metrics_2:\n",
    "            block_means = filtered_subset.groupby(\"epoch_block\")[[metric]].mean()\n",
    "            middle_epochs = filtered_subset.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "            plt.plot(middle_epochs, block_means, label=metric.replace(\"_\", \" \").title())\n",
    "        plt.title(f\"Metrics per Epoch - Game Type {game_type}, LR {lr}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Mean value\")\n",
    "        plt.legend()\n",
    "        #plt.xlim([0, 100])\n",
    "        plt.tight_layout()\n",
    "\n",
    "        filename_2 = f\"rewarded_metrics_agent2_g{game_type}_lr{str(lr).replace('.', 'p')}_smoothed_{N}.png\"\n",
    "        filepath_2 = os.path.join(smoothed_figures_dir, filename_2)\n",
    "        plt.savefig(filepath_2)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0080c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print agent metrics vs epoch\n",
    "\n",
    "for game_type in unique_game_type:\n",
    "    for lr in unique_lr:\n",
    "        filtered_subset = df[(df[\"game_type\"] == game_type) & (df[\"lr\"] == lr)]\n",
    "        filtered_subset[\"epoch_block\"] = (filtered_subset[\"epoch\"] // N)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for metric in action_types_1:\n",
    "            block_means = filtered_subset.groupby(\"epoch_block\")[[metric]].mean()\n",
    "            middle_epochs = filtered_subset.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "            plt.plot(middle_epochs, block_means, label=metric.replace(\"_\", \" \").title())\n",
    "        plt.title(f\"Metrics per Epoch - Game Type {game_type}, LR {lr}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Mean value\")\n",
    "        plt.legend()\n",
    "        #plt.xlim([0, 100])\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename_1 = f\"metrics_agent1_g{game_type}_lr{str(lr).replace('.', 'p')}_smoothed_{N}.png\"\n",
    "        filepath_1 = os.path.join(smoothed_figures_dir, filename_1)\n",
    "        plt.savefig(filepath_1)\n",
    "        plt.close()\n",
    "    \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for metric in action_types_2:\n",
    "            block_means = filtered_subset.groupby(\"epoch_block\")[[metric]].mean()\n",
    "            middle_epochs = filtered_subset.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "            plt.plot(middle_epochs, block_means, label=metric.replace(\"_\", \" \").title())\n",
    "        plt.title(f\"Metrics per Epoch - Game Type {game_type}, LR {lr}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Mean value\")\n",
    "        plt.legend()\n",
    "        #plt.xlim([0, 100])\n",
    "        plt.tight_layout()\n",
    "\n",
    "        filename_2 = f\"metrics_agent2_g{game_type}_lr{str(lr).replace('.', 'p')}_smoothed_{N}.png\"\n",
    "        filepath_2 = os.path.join(smoothed_figures_dir, filename_2)\n",
    "        plt.savefig(filepath_2)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18065ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Pure total reward vs epoch\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for game_type in unique_game_type:\n",
    "        game_type_filtered = subset[subset[\"game_type\"] == game_type]\n",
    "    \n",
    "        # Filtrar por tasa de aprendizaje\n",
    "        for lr in unique_lr:\n",
    "            lr_filtered = game_type_filtered[game_type_filtered[\"lr\"] == lr]\n",
    "            lr_filtered[\"epoch_block\"] = (lr_filtered[\"epoch\"] // N)\n",
    "\n",
    "            # Calcular la media de recompensa por bloque\n",
    "            block_means = lr_filtered.groupby(\"epoch_block\")[\"pure_reward_total\"].mean()\n",
    "            middle_epochs = lr_filtered.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "\n",
    "            if game_type == 0:\n",
    "                label = f\"Competitive\"\n",
    "                color = \"red\"\n",
    "            else:\n",
    "                label = f\"Cooperative\"\n",
    "                color = \"orange\"\n",
    "\n",
    "            plt.plot(middle_epochs, block_means, label=label, color=color)\n",
    "    \n",
    "    # Añadir detalles\n",
    "    plt.xlabel(\"Epochs\", fontsize=20)\n",
    "    #plt.xlim([0,100])\n",
    "    plt.title(f\"Pure Reward vs Epoch\\nAttitude {attitude}\")\n",
    "\n",
    "    plt.ylabel(\"Mean total reward\", fontsize=20)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    #plt.ylim(-140, 320)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    sanitized_attitude = attitude.replace('.', 'p')\n",
    "    filename = f\"pure_reward_attitude_{sanitized_attitude}_smoothed_{N}.png\"\n",
    "    filepath = os.path.join(smoothed_figures_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62509378",
   "metadata": {},
   "source": [
    "### Action types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e20b9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual attitudes found: ['1.0_0.0']\n",
      "Figures created with metrics averaged by individual attitude (regardless of which agent has the attitude).\n"
     ]
    }
   ],
   "source": [
    "# Print agent metrics vs epoch averaged by individual attitude\n",
    "# First, create individual attitude keys for each agent\n",
    "df['attitude_agent_1'] = df['alpha_1'].astype(str) + '_' + df['beta_1'].astype(str)\n",
    "df['attitude_agent_2'] = df['alpha_2'].astype(str) + '_' + df['beta_2'].astype(str)\n",
    "\n",
    "# Get unique individual attitudes\n",
    "unique_individual_attitudes = set()\n",
    "for attitude in unique_attitudes:\n",
    "    att_parts = attitude.split('_') \n",
    "    unique_individual_attitudes.add(f\"{att_parts[0]}_{att_parts[1]}\")  # agent 1 attitude\n",
    "    unique_individual_attitudes.add(f\"{att_parts[2]}_{att_parts[3]}\")  # agent 2 attitude\n",
    "\n",
    "print(f\"Individual attitudes found: {sorted(unique_individual_attitudes)}\")\n",
    "\n",
    "# Now create plots for each individual attitude\n",
    "for individual_attitude in unique_individual_attitudes:\n",
    "    att_parts = individual_attitude.split('_')\n",
    "    alpha = float(att_parts[0])\n",
    "    beta = float(att_parts[1])\n",
    "    \n",
    "    # Calculate degree for title\n",
    "    if alpha == 0 and beta == 0:\n",
    "        degree = 0\n",
    "    else:\n",
    "        degree = np.degrees(np.arctan2(beta, alpha)) % 360\n",
    "\n",
    "    for lr in unique_lr:\n",
    "        # Filter data where either agent 0 or agent 1 has this attitude\n",
    "        mask_agent_1 = (df['attitude_agent_1'] == individual_attitude)\n",
    "        mask_agent_2 = (df['attitude_agent_2'] == individual_attitude)\n",
    "        mask_conditions = (df[\"lr\"] == lr)\n",
    "\n",
    "        filtered_subset = df[mask_conditions & (mask_agent_1 | mask_agent_2)].copy()\n",
    "\n",
    "        if len(filtered_subset) > 0:\n",
    "            filtered_subset[\"epoch_block\"] = (filtered_subset[\"epoch\"] // N)\n",
    "\n",
    "            # Create a new dataframe with attitude-specific metrics\n",
    "            attitude_metrics = []\n",
    "\n",
    "            for _, row in filtered_subset.iterrows():\n",
    "                # Check if agent 1 has this attitude\n",
    "                if row['attitude_agent_1'] == individual_attitude:\n",
    "                    # Agent 1 has this attitude, add his metrics\n",
    "                    for metric in action_types_1:\n",
    "                        base_metric = metric.replace(\"_ai_rl_1\", \"\")\n",
    "                        attitude_metrics.append({\n",
    "                            'epoch': row['epoch'],\n",
    "                            'epoch_block': row['epoch_block'],\n",
    "                            'metric': base_metric,\n",
    "                            'value': row[metric]\n",
    "                        })\n",
    "\n",
    "                # Check if agent 2 has this attitude\n",
    "                if row['attitude_agent_2'] == individual_attitude:\n",
    "                    # Agent 2 has this attitude, add his metrics\n",
    "                    for metric in action_types_2:\n",
    "                        base_metric = metric.replace(\"_ai_rl_2\", \"\")\n",
    "                        attitude_metrics.append({\n",
    "                            'epoch': row['epoch'],\n",
    "                            'epoch_block': row['epoch_block'],\n",
    "                            'metric': base_metric,\n",
    "                            'value': row[metric]\n",
    "                        })\n",
    "\n",
    "            if attitude_metrics:  # Only create plot if we have data\n",
    "                attitude_df = pd.DataFrame(attitude_metrics)\n",
    "\n",
    "                # Create plot for this attitude\n",
    "                plt.figure(figsize=(12, 6))\n",
    "\n",
    "                # Plot each metric (now properly averaged across agents)\n",
    "                for metric in attitude_df['metric'].unique():\n",
    "                    metric_data = attitude_df[attitude_df['metric'] == metric]\n",
    "                    block_means = metric_data.groupby(\"epoch_block\")[\"value\"].mean()\n",
    "                    middle_epochs = metric_data.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "                    plt.plot(middle_epochs, block_means,\n",
    "                             label=action_types_labels[metric],\n",
    "                             color=action_types_colors[metric])\n",
    "\n",
    "                plt.xlabel(\"Episodes\", fontsize=20)\n",
    "                plt.title(f\"Attitude {individual_attitude} ({degree:.1f}°) - LR {lr}\")\n",
    "\n",
    "                plt.ylabel(\"Number of times the action was taken\", fontsize=20)\n",
    "                plt.legend(fontsize=20, loc='upper left', frameon=True, framealpha=0.9, edgecolor='black')\n",
    "                plt.xticks(fontsize=18)\n",
    "                plt.yticks(fontsize=18)\n",
    "                #plt.ylim(0, 135)\n",
    "                plt.tight_layout()\n",
    "\n",
    "                sanitized_attitude = individual_attitude.replace('.', 'p')\n",
    "                filename = f\"action_types_attitude_{sanitized_attitude}_lr{str(lr).replace('.', 'p')}_smoothed_{N}.png\"\n",
    "                filepath = os.path.join(smoothed_figures_dir, filename)\n",
    "                plt.savefig(filepath)\n",
    "                plt.close()\n",
    "\n",
    "print(\"Figures created with metrics averaged by individual attitude (regardless of which agent has the attitude).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2d6f1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures created with metrics averaged over all other agent attitudes.\n",
      "Files saved with '_avg_' in the filename to distinguish from individual combination plots.\n"
     ]
    }
   ],
   "source": [
    "# Also create combined plots showing both agents together for each attitude\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    att_parts = attitude.split('_')\n",
    "    att1_title = f\"{att_parts[0]}_{att_parts[1]}\"\n",
    "    att2_title = f\"{att_parts[2]}_{att_parts[3]}\"\n",
    "\n",
    "    for game_type in unique_game_type:\n",
    "            for lr in unique_lr:\n",
    "                filtered_subset = subset[(subset[\"game_type\"] == game_type) & (subset[\"lr\"] == lr)]\n",
    "                filtered_subset[\"epoch_block\"] = (filtered_subset[\"epoch\"] // N)\n",
    "\n",
    "                # Create combined plot for all metrics\n",
    "                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "                \n",
    "                # Agent 1 metrics\n",
    "                for metric in action_types_1:\n",
    "                    block_means = filtered_subset.groupby(\"epoch_block\")[metric].mean()\n",
    "                    middle_epochs = filtered_subset.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "                    metric_without_agent = metric.replace(\"_ai_rl_1\", \"\")\n",
    "                    ax1.plot(middle_epochs, block_means,\n",
    "                                 label=action_types_labels[metric_without_agent],\n",
    "                                 color=action_types_colors[metric_without_agent])\n",
    "\n",
    "                ax1.set_ylabel(\"Number of times the action was taken\", fontsize=18)\n",
    "                # Legend outside the plot (to the right)\n",
    "                ax1.legend(\n",
    "                    fontsize=16,\n",
    "                    loc='center left',\n",
    "                    bbox_to_anchor=(1.02, 0.5),   # fuera del gráfico, centrada verticalmente\n",
    "                    frameon=True,\n",
    "                    framealpha=0.9,\n",
    "                    edgecolor='black'\n",
    "                )\n",
    "                #ax1.set_ylim(0, 145)\n",
    "                \n",
    "                # Agent 2 metrics\n",
    "                for metric in action_types_2:\n",
    "                    block_means = filtered_subset.groupby(\"epoch_block\")[metric].mean()\n",
    "                    middle_epochs = filtered_subset.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "                    metric_without_agent = metric.replace(\"_ai_rl_2\", \"\")\n",
    "                    ax2.plot(middle_epochs, block_means,\n",
    "                                 label=action_types_labels[metric_without_agent],\n",
    "                                 color=action_types_colors[metric_without_agent])\n",
    "\n",
    "                ax2.set_xlabel(\"Episodes\", fontsize=20)\n",
    "                ax2.set_ylabel(\"Number of times the action was taken\", fontsize=18)\n",
    "                # Legend outside the plot (to the right)\n",
    "                ax2.legend(\n",
    "                    fontsize=16,\n",
    "                    loc='center left',\n",
    "                    bbox_to_anchor=(1.02, 0.5),   # fuera del gráfico, centrada verticalmente\n",
    "                    frameon=True,\n",
    "                    framealpha=0.9,\n",
    "                    edgecolor='black'\n",
    "                )\n",
    "                #ax2.set_ylim(0, 145)\n",
    "\n",
    "                plt.tight_layout(rect=[0, 0, 0.99, 1])  # deja espacio a la derecha\n",
    "                \n",
    "                sanitized_attitude = attitude.replace('.', 'p')\n",
    "                filename_combined = f\"action_types_combined_avg_g{game_type}_lr{str(lr).replace('.', 'p')}_attitude_{sanitized_attitude}_smoothed_{N}.png\"\n",
    "                filepath_combined = os.path.join(smoothed_figures_dir, filename_combined)\n",
    "                plt.savefig(filepath_combined)\n",
    "                plt.close()\n",
    "\n",
    "print(\"Figures created with metrics averaged over all other agent attitudes.\")\n",
    "print(\"Files saved with '_avg_' in the filename to distinguish from individual combination plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc8c23",
   "metadata": {},
   "source": [
    "### Result events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4ee8721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual attitudes found: ['1.0_0.0']\n",
      "Figures created with metrics averaged by individual attitude (regardless of which agent has the attitude).\n"
     ]
    }
   ],
   "source": [
    "# Print agent metrics vs epoch averaged by individual attitude\n",
    "# First, create individual attitude keys for each agent\n",
    "df['attitude_agent_1'] = df['alpha_1'].astype(str) + '_' + df['beta_1'].astype(str)\n",
    "df['attitude_agent_2'] = df['alpha_2'].astype(str) + '_' + df['beta_2'].astype(str)\n",
    "\n",
    "# Get unique individual attitudes\n",
    "unique_individual_attitudes = set()\n",
    "for attitude in unique_attitudes:\n",
    "    att_parts = attitude.split('_') \n",
    "    unique_individual_attitudes.add(f\"{att_parts[0]}_{att_parts[1]}\")  # agent 1 attitude\n",
    "    unique_individual_attitudes.add(f\"{att_parts[2]}_{att_parts[3]}\")  # agent 2 attitude\n",
    "\n",
    "print(f\"Individual attitudes found: {sorted(unique_individual_attitudes)}\")\n",
    "\n",
    "# Now create plots for each individual attitude\n",
    "for individual_attitude in unique_individual_attitudes:\n",
    "    att_parts = individual_attitude.split('_')\n",
    "    alpha = float(att_parts[0])\n",
    "    beta = float(att_parts[1])\n",
    "    \n",
    "    # Calculate degree for title\n",
    "    if alpha == 0 and beta == 0:\n",
    "        degree = 0\n",
    "    else:\n",
    "        degree = np.degrees(np.arctan2(beta, alpha)) % 360\n",
    "\n",
    "    for game_type in unique_game_type:\n",
    "        for lr in unique_lr:\n",
    "            # Filter data where either agent 0 or agent 1 has this attitude\n",
    "            mask_agent_1 = (df['attitude_agent_1'] == individual_attitude)\n",
    "            mask_agent_2 = (df['attitude_agent_2'] == individual_attitude)\n",
    "            mask_conditions = (df[\"game_type\"] == game_type) & (df[\"lr\"] == lr)\n",
    "\n",
    "            filtered_subset = df[mask_conditions & (mask_agent_1 | mask_agent_2)].copy()\n",
    "\n",
    "            if len(filtered_subset) > 0:\n",
    "                filtered_subset[\"epoch_block\"] = (filtered_subset[\"epoch\"] // N)\n",
    "\n",
    "                # Create a new dataframe with attitude-specific metrics\n",
    "                attitude_metrics = []\n",
    "\n",
    "                for _, row in filtered_subset.iterrows():\n",
    "                    # Check if agent 1 has this attitude\n",
    "                    if row['attitude_agent_1'] == individual_attitude:\n",
    "                        # Agent 1 has this attitude, add his metrics\n",
    "                        for metric in rewarded_metrics_1:\n",
    "                            base_metric = metric.replace(\"_ai_rl_1\", \"\")\n",
    "                            attitude_metrics.append({\n",
    "                                'epoch': row['epoch'],\n",
    "                                'epoch_block': row['epoch_block'],\n",
    "                                'metric': base_metric,\n",
    "                                'value': row[metric]\n",
    "                            })\n",
    "\n",
    "                    # Check if agent 2 has this attitude\n",
    "                    if row['attitude_agent_2'] == individual_attitude:\n",
    "                        # Agent 2 has this attitude, add his metrics\n",
    "                        for metric in rewarded_metrics_2:\n",
    "                            base_metric = metric.replace(\"_ai_rl_2\", \"\")\n",
    "                            attitude_metrics.append({\n",
    "                                'epoch': row['epoch'],\n",
    "                                'epoch_block': row['epoch_block'],\n",
    "                                'metric': base_metric,\n",
    "                                'value': row[metric]\n",
    "                            })\n",
    "\n",
    "                if attitude_metrics:  # Only create plot if we have data\n",
    "                    attitude_df = pd.DataFrame(attitude_metrics)\n",
    "\n",
    "                    # Create plot for this attitude\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "\n",
    "                    # Plot each metric (now properly averaged across agents)\n",
    "                    for metric in attitude_df['metric'].unique():\n",
    "                        metric_data = attitude_df[attitude_df['metric'] == metric]\n",
    "                        block_means = metric_data.groupby(\"epoch_block\")[\"value\"].mean()\n",
    "                        middle_epochs = metric_data.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "                        plt.plot(middle_epochs, block_means,\n",
    "                                 label=rewarded_metric_labels[metric],\n",
    "                                 color=rewarded_metric_colors[metric])\n",
    "\n",
    "                    plt.xlabel(\"Episodes\", fontsize=20)\n",
    "                    plt.title(f\"Attitude {individual_attitude} ({degree:.1f}°) - LR {lr}\")\n",
    "\n",
    "                    plt.ylabel(\"Number of times the action was taken\", fontsize=20)\n",
    "                    plt.legend(fontsize=20, loc='upper left', frameon=True, framealpha=0.9, edgecolor='black')\n",
    "                    plt.xticks(fontsize=18)\n",
    "                    plt.yticks(fontsize=18)\n",
    "                    #plt.ylim(0, 10)\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    sanitized_attitude = individual_attitude.replace('.', 'p')\n",
    "                    filename = f\"rewarded_metrics_attitude_{sanitized_attitude}_lr{str(lr).replace('.', 'p')}_smoothed_{N}.png\"\n",
    "                    filepath = os.path.join(smoothed_figures_dir, filename)\n",
    "                    plt.savefig(filepath)\n",
    "                    plt.close()\n",
    "\n",
    "print(\"Figures created with metrics averaged by individual attitude (regardless of which agent has the attitude).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eadd77af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures created with metrics averaged over all other agent attitudes.\n",
      "Files saved with '_avg_' in the filename to distinguish from individual combination plots.\n"
     ]
    }
   ],
   "source": [
    "# Also create combined plots showing both agents together for each attitude\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    att_parts = attitude.split('_')\n",
    "    att1_title = f\"{att_parts[0]}_{att_parts[1]}\"\n",
    "    att2_title = f\"{att_parts[2]}_{att_parts[3]}\"\n",
    "\n",
    "    for game_type in unique_game_type:\n",
    "        for lr in unique_lr:\n",
    "            filtered_subset = subset[(subset[\"game_type\"] == game_type) & (subset[\"lr\"] == lr)]\n",
    "            filtered_subset[\"epoch_block\"] = (filtered_subset[\"epoch\"] // N)\n",
    "\n",
    "            # Create combined plot for all metrics\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "            \n",
    "            # Agent 1 metrics\n",
    "            for metric in rewarded_metrics_1:\n",
    "                block_means = filtered_subset.groupby(\"epoch_block\")[metric].mean()\n",
    "                middle_epochs = filtered_subset.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "                metric_without_agent = metric.replace(\"_ai_rl_1\", \"\")\n",
    "                ax1.plot(middle_epochs, block_means,\n",
    "                             label=rewarded_metric_labels[metric_without_agent],\n",
    "                             color=rewarded_metric_colors[metric_without_agent])\n",
    "\n",
    "            ax1.set_ylabel(\"Number of times the action was taken\", fontsize=18)\n",
    "            ax1.legend(fontsize=20, loc='upper left', frameon=True, framealpha=0.9, edgecolor='black')\n",
    "            #ax1.set_ylim(0, 145)\n",
    "            \n",
    "            # Agent 2 metrics\n",
    "            for metric in rewarded_metrics_2:\n",
    "                block_means = filtered_subset.groupby(\"epoch_block\")[metric].mean()\n",
    "                middle_epochs = filtered_subset.groupby(\"epoch_block\")[\"epoch\"].median()\n",
    "                metric_without_agent = metric.replace(\"_ai_rl_2\", \"\")\n",
    "                ax2.plot(middle_epochs, block_means,\n",
    "                             label=rewarded_metric_labels[metric_without_agent],\n",
    "                             color=rewarded_metric_colors[metric_without_agent])\n",
    "\n",
    "            ax2.set_xlabel(\"Episodes\", fontsize=20)\n",
    "            ax2.set_ylabel(\"Number of times the action was taken\", fontsize=18)\n",
    "            ax2.legend(fontsize=20, loc='upper left', frameon=True, framealpha=0.9, edgecolor='black')\n",
    "            #ax2.set_ylim(0, 145)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            sanitized_attitude = attitude.replace('.', 'p')\n",
    "            filename_combined = f\"rewarded_metrics_combined_avg_g{game_type}_lr{str(lr).replace('.', 'p')}_attitude_{sanitized_attitude}_smoothed_{N}.png\"\n",
    "            filepath_combined = os.path.join(smoothed_figures_dir, filename_combined)\n",
    "            plt.savefig(filepath_combined)\n",
    "            plt.close()\n",
    "\n",
    "print(\"Figures created with metrics averaged over all other agent attitudes.\")\n",
    "print(\"Files saved with '_avg_' in the filename to distinguish from individual combination plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc80d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cee5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooked_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
