{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b5305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from spoiled_broth.rl.game_env import GameEnv\n",
    "from pathlib import Path\n",
    "import supersuit as ss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model = 'PPO'\n",
    "map_nr = 1\n",
    "\n",
    "# Define reward weights\n",
    "reward_weights = {\n",
    "    \"ai_rl_1\": (0.5, 0.5),  # agent 1 is cooperative\n",
    "    \"ai_rl_2\": (0.8, -0.2),  # agent 2 is competitive\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6478af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(f\"spoiled_broth/rl/saved_models/DTDE/{rl_model}_spoiled_broth-map_{map_nr}/\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bbcd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(config):\n",
    "    # Your existing GameEnv class goes here\n",
    "    return GameEnv(**config)\n",
    "\n",
    "#def env_creator_2(config):\n",
    "#    env = GameEnv(reward_weights=reward_weights, map_nr=map_nr)\n",
    "#    env = ss.pad_observations_v0(env)\n",
    "#    env = ss.pad_action_space_v0(env)\n",
    "#    return PettingZooEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4debe51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "#ray.shutdown()\n",
    "#ray.init(ignore_reinit_error=True)\n",
    "#\n",
    "## Define the environment name\n",
    "#env_name = \"spoiled_broth_multiagent\"\n",
    "\n",
    "# Register the environment with RLLib\n",
    "register_env(\"spoiled_broth\", lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
    "\n",
    "# Define separate policies for each agent\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    return f\"policy_{agent_id}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration for multi-agent training\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"spoiled_broth\",\n",
    "        env_config={\n",
    "            \"reward_weights\": reward_weights,\n",
    "            \"map_nr\": map_nr\n",
    "        },\n",
    "        clip_actions=True\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"shared_policy\": (\n",
    "                None,\n",
    "                env_creator({}).observation_space(\"ai_rl_1\"),\n",
    "                env_creator({}).action_space(\"ai_rl_1\"),\n",
    "                {}\n",
    "            )\n",
    "        },\n",
    "        policy_mapping_fn=lambda agent_id, *args, **kwargs: \"shared_policy\",\n",
    "        policies_to_train=[\"shared_policy\"]\n",
    "    )\n",
    "    .resources(num_gpus=1)  # Set to 1 if you have a GPU\n",
    "    .env_runners(num_env_runners=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b210f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 12:22:58,110\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-05-17 12:22:58,115\tWARNING algorithm_config.py:4968 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-05-17 12:23:00,172\tINFO worker.py:1888 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m Weighted rewards:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m Agent 1: (0.5, 0.5)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m Agent 2: (0.8, -0.2)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m 2025-05-17 12:23:03,982\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-05-17 12:23:04,211\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-05-17 12:23:05,657\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m 2025-05-17 12:23:05,668\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: policy_mapping_fn() missing 1 required positional argument: 'worker'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m                      ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 222, in sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m         num_timesteps=num_timesteps,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     ...<2 lines>...\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m         force_reset=force_reset,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     )\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 277, in _sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 552, in _reset_envs\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     episodes[env_index].add_env_reset(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m         observations=observations[env_index],\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m         infos=infos[env_index],\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     )\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     ^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_episode.py\", line 332, in add_env_reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     module_id=self.module_for(agent_id),\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m               ~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_episode.py\", line 1048, in module_for\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m     ] = self.agent_to_module_mapping_fn(agent_id, self)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540011)\u001b[0m TypeError: policy_mapping_fn() missing 1 required positional argument: 'worker'\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-17 12:23:09,128 E 1539880 1539917] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-17_12-22-58_184284_1539600 is over 95% full, available space: 36.2585 GB; capacity: 878.621 GB. Object creation will fail if spilling is required.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m 2025-05-17 12:23:03,991\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m 2025-05-17 12:23:05,668\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: policy_mapping_fn() missing 1 required positional argument: 'worker'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m                      ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 222, in sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m         num_timesteps=num_timesteps,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     ...<2 lines>...\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m         force_reset=force_reset,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     )\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 277, in _sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 552, in _reset_envs\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     episodes[env_index].add_env_reset(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m         observations=observations[env_index],\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m         infos=infos[env_index],\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     ^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_episode.py\", line 332, in add_env_reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     module_id=self.module_for(agent_id),\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m               ~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_episode.py\", line 1048, in module_for\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m     ] = self.agent_to_module_mapping_fn(agent_id, self)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1540012)\u001b[0m TypeError: policy_mapping_fn() missing 1 required positional argument: 'worker'\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-17 12:23:19,140 E 1539880 1539917] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-17_12-22-58_184284_1539600 is over 95% full, available space: 36.2585 GB; capacity: 878.621 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-17 12:23:29,152 E 1539880 1539917] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-17_12-22-58_184284_1539600 is over 95% full, available space: 36.2585 GB; capacity: 878.621 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-17 12:23:39,164 E 1539880 1539917] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-17_12-22-58_184284_1539600 is over 95% full, available space: 36.2585 GB; capacity: 878.621 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-17 12:23:49,177 E 1539880 1539917] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-17_12-22-58_184284_1539600 is over 95% full, available space: 36.2585 GB; capacity: 878.621 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-17 12:23:59,190 E 1539880 1539917] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-17_12-22-58_184284_1539600 is over 95% full, available space: 36.2585 GB; capacity: 878.621 GB. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "# Build and train the algorithm\n",
    "algo = config.build()\n",
    "\n",
    "for i in range(10):  # Run for 10 iterations\n",
    "    result = algo.train()\n",
    "    print(f\"Iteration {i}: reward={result['episode_reward_mean']}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save()\n",
    "        print(f\"Checkpoint saved in {checkpoint_dir}\")\n",
    "        eval_result = algo.evaluate()\n",
    "        print(f\"Evaluation reward: {eval_result['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb58961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooked",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
