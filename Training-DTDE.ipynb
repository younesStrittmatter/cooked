{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b5305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from spoiled_broth.rl.game_env import GameEnv\n",
    "from pathlib import Path\n",
    "import supersuit as ss\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model = 'PPO'\n",
    "map_nr = 1\n",
    "\n",
    "# Define reward weights\n",
    "reward_weights = {\n",
    "    \"ai_rl_1\": (0.5, 0.5),  # agent 1 is full cooperative\n",
    "    \"ai_rl_2\": (0.9, 0.1),  # agent 2 is less cooperative\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6478af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "brigit = \"/mnt/lustre/home/samuloza\"\n",
    "base_save_dir = f\"{brigit}/data/samuel_lozano/cooked/saved_models/map_{map_nr}/\"\n",
    "Path(base_save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bbcd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(config):\n",
    "    # Your existing GameEnv class goes here\n",
    "    return GameEnv(**config)\n",
    "\n",
    "#def env_creator_2(config):\n",
    "#    env = GameEnv(reward_weights=reward_weights, map_nr=map_nr)\n",
    "#    env = ss.pad_observations_v0(env)\n",
    "#    env = ss.pad_action_space_v0(env)\n",
    "#    return PettingZooEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4debe51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 20:06:48,609\tINFO worker.py:1888 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "#os.environ[\"RAY_TMPDIR\"] = f\"/data/samuel_lozano/tmp_ray/\"\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True#, _temp_dir=os.environ[\"RAY_TMPDIR\"]\n",
    "         )\n",
    "\n",
    "# Register the environment with RLLib\n",
    "register_env(\"spoiled_broth\", lambda config: ParallelPettingZooEnv(env_creator(config)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f437163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (0.5, 0.5)\n",
      "\n",
      "Agent 2: (0.9, 0.1)\n",
      "\n",
      "Observation space: Box(0.0, 50.0, (1668,), float32)\n",
      "Sample observation shape: (1668,)\n",
      "Sample observation min/max: 0.0, 50.0\n"
     ]
    }
   ],
   "source": [
    "# Verify observation space consistency\n",
    "def verify_observation_space():\n",
    "    test_env = env_creator({\"reward_weights\": reward_weights, \"map_nr\": 1})\n",
    "    obs_space = test_env.observation_space(\"ai_rl_1\")\n",
    "    sample_obs = test_env.reset()[0][\"ai_rl_1\"]\n",
    "    \n",
    "    print(f\"Observation space: {obs_space}\")\n",
    "    print(f\"Sample observation shape: {sample_obs.shape}\")\n",
    "    print(f\"Sample observation min/max: {np.min(sample_obs)}, {np.max(sample_obs)}\")\n",
    "    \n",
    "    assert obs_space.contains(sample_obs), \"Observation doesn't match observation space!\"\n",
    "    test_env.close()\n",
    "    return obs_space\n",
    "\n",
    "# Get verified observation space\n",
    "obs_space = verify_observation_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bd9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate policies for each agent\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    return f\"policy_{agent_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a6d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration for multi-agent training\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True\n",
    "    )\n",
    "    .environment(\n",
    "        env=\"spoiled_broth\",\n",
    "        env_config={\n",
    "            \"reward_weights\": reward_weights,\n",
    "            \"map_nr\": map_nr\n",
    "        },\n",
    "        clip_actions=True,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_ai_rl_1\": (\n",
    "                None,  # Use default PPO policy\n",
    "                env_creator({\"map_nr\": 1}).observation_space(\"ai_rl_1\"),\n",
    "                env_creator({\"map_nr\": 1}).action_space(\"ai_rl_1\"),\n",
    "                {}\n",
    "            ),\n",
    "            \"policy_ai_rl_2\": (\n",
    "                None,  # Use default PPO policy\n",
    "                env_creator({\"map_nr\": 1}).observation_space(\"ai_rl_2\"),\n",
    "                env_creator({\"map_nr\": 1}).action_space(\"ai_rl_2\"),\n",
    "                {}\n",
    "            )\n",
    "        },\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        policies_to_train=[\"policy_ai_rl_1\", \"policy_ai_rl_2\"]\n",
    "    )\n",
    "    .resources(num_gpus=1)  # Set to 1 if you have a GPU\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        minibatch_size=500,\n",
    "        num_epochs=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370781c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error building algorithm: '<' not supported between instances of 'Version' and 'ValueError'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Version' and 'ValueError'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Build algorithm with error handling\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     algo = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     full_run_dir = algo.logdir\n\u001b[32m      5\u001b[39m     NAME_RAY = Path(full_run_dir).name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cooked_11/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:997\u001b[39m, in \u001b[36mAlgorithmConfig.build_algo\u001b[39m\u001b[34m(self, env, logger_creator, use_copy)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.algo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    995\u001b[39m     algo_class = get_trainable_cls(\u001b[38;5;28mself\u001b[39m.algo_class)\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cooked_11/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:465\u001b[39m, in \u001b[36mAlgorithm.__init__\u001b[39m\u001b[34m(self, config, env, logger_creator, **kwargs)\u001b[39m\n\u001b[32m    462\u001b[39m     config.environment(env)\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# Validate and freeze our AlgorithmConfig object (no more changes possible).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m config.freeze()\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# Convert `env` provided in config into a concrete env creator callable, which\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# takes an EnvContext (config dict) as arg and returning an RLlib supported Env\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[38;5;66;03m# type (e.g. a gym.Env).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cooked_11/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:284\u001b[39m, in \u001b[36mPPOConfig.validate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;129m@override\u001b[39m(AlgorithmConfig)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    283\u001b[39m     \u001b[38;5;66;03m# Call super's validation method.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Synchronous sampling, on-policy/PPO algos -> Check mismatches between\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# `rollout_fragment_length` and `train_batch_size_per_learner` to avoid user\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# confusion.\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# TODO (sven): Make rollout_fragment_length a property and create a private\u001b[39;00m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m#  attribute to store (possibly) user provided value (or \"auto\") in. Deprecate\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m#  `self.get_rollout_fragment_length()`.\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.validate_train_batch_size_vs_rollout_fragment_length()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cooked_11/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:953\u001b[39m, in \u001b[36mAlgorithmConfig.validate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_env_runner_settings()\n\u001b[32m    952\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_callbacks_settings()\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_framework_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_resources_settings()\n\u001b[32m    955\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_multi_agent_settings()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cooked_11/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:4798\u001b[39m, in \u001b[36mAlgorithmConfig._validate_framework_settings\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4788\u001b[39m     \u001b[38;5;28mself\u001b[39m._value_error(\n\u001b[32m   4789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot use `framework=tf` with the new API stack! Either switch to tf2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4790\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m via `config.framework(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtf2\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)` OR disable the new API stack via \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4791\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`config.api_stack(enable_rl_module_and_learner=False)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4792\u001b[39m     )\n\u001b[32m   4794\u001b[39m \u001b[38;5;66;03m# Check if torch framework supports torch.compile.\u001b[39;00m\n\u001b[32m   4795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4796\u001b[39m     _torch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4797\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework_str == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4798\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_torch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43mTORCH_COMPILE_REQUIRED_VERSION\u001b[49m\n\u001b[32m   4799\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.torch_compile_learner \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.torch_compile_worker)\n\u001b[32m   4800\u001b[39m ):\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m._value_error(\u001b[33m\"\u001b[39m\u001b[33mtorch.compile is only supported from torch 2.0.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4803\u001b[39m \u001b[38;5;66;03m# Make sure the Learner's torch-what-to-compile setting is supported.\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'Version' and 'ValueError'"
     ]
    }
   ],
   "source": [
    "# Build algorithm with error handling\n",
    "try:\n",
    "    algo = config.build_algo()\n",
    "    full_run_dir = algo.logdir\n",
    "    NAME_RAY = Path(full_run_dir).name\n",
    "    print(f\"Algorithm built successfully: {NAME_RAY}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error building algorithm: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8b210f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NAME_RAY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save training config\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_dir = Path(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_save_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mNAME_RAY\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m training_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m shutil.copytree(full_run_dir, training_dir, dirs_exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'NAME_RAY' is not defined"
     ]
    }
   ],
   "source": [
    "# Save training config\n",
    "training_dir = Path(f\"{base_save_dir}{NAME_RAY}\")\n",
    "training_dir.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copytree(full_run_dir, training_dir, dirs_exist_ok=True)\n",
    "\n",
    "def safe_config_dict(d):\n",
    "    def make_serializable(o):\n",
    "        try:\n",
    "            json.dumps(o)\n",
    "            return o\n",
    "        except TypeError:\n",
    "            return str(o)\n",
    "\n",
    "    return {\n",
    "        k: make_serializable(v)\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "\n",
    "config_path = f\"{training_dir}/config.txt\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(safe_config_dict(config.to_dict()), f, indent=4)\n",
    "\n",
    "for i in range(30): \n",
    "    result = algo.train()\n",
    "    print(f\"Iteration {i}:\")\n",
    "    print(f\"  Agent 1 reward: {result['env_runners']['module_episode_returns_mean'].get('policy_ai_rl_1', 0)}\")\n",
    "    print(f\"  Agent 2 reward: {result['env_runners']['module_episode_returns_mean'].get('policy_ai_rl_2', 0)}\")\n",
    "    print(f\"  Total reward: {result['env_runners']['episode_return_mean']}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_obtained = algo.save()\n",
    "        checkpoint_path = Path(checkpoint_obtained.checkpoint.path)\n",
    "        \n",
    "        custom_checkpoint_dir = Path(f\"{training_dir}/Checkpoint_{i}\")\n",
    "        custom_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for item in checkpoint_path.iterdir():\n",
    "            dst = custom_checkpoint_dir / item.name\n",
    "            if item.is_dir():\n",
    "                shutil.copytree(item, dst, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(item, dst)\n",
    "\n",
    "        print(f\"Checkpoint {i} saved in {custom_checkpoint_dir}\")\n",
    "        \n",
    "        #eval_result = algo.evaluate()\n",
    "        #print(f\"Evaluation reward: {eval_result['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64f064",
   "metadata": {},
   "source": [
    "# Reload params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1477dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b037174",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_map_nr = 1\n",
    "\n",
    "training_id = \"PPO_spoiled_broth_2025-05-18_22-22-47j6l54y_m\"\n",
    "\n",
    "checkpoint_nr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf4aacc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:48:33,277\tINFO trainable.py:160 -- Trainable.setup took 14.885 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgentEnvRunner' object has no attribute 'get_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m reload_checkpoint_dir = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/data/samuel_lozano/cooked/saved_models/map_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreload_map_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/Checkpoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m reload_algo = Algorithm.from_checkpoint(reload_checkpoint_dir)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m policy_module_1 = \u001b[43mreload_algo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpolicy_ai_rl_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m policy_module_2 = reload_algo.get_policy(\u001b[33m\"\u001b[39m\u001b[33mpolicy_ai_rl_2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Comparar pesos (si estás usando PyTorch)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/algorithms/algorithm.py:2389\u001b[39m, in \u001b[36mAlgorithm.get_policy\u001b[39m\u001b[34m(self, policy_id)\u001b[39m\n\u001b[32m   2382\u001b[39m \u001b[38;5;129m@OldAPIStack\u001b[39m\n\u001b[32m   2383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_id: PolicyID = DEFAULT_POLICY_ID) -> Policy:\n\u001b[32m   2384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return policy for the specified id, or None.\u001b[39;00m\n\u001b[32m   2385\u001b[39m \n\u001b[32m   2386\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   2387\u001b[39m \u001b[33;03m        policy_id: ID of the policy to return.\u001b[39;00m\n\u001b[32m   2388\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m(policy_id)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MultiAgentEnvRunner' object has no attribute 'get_policy'"
     ]
    }
   ],
   "source": [
    "# Restaura el algoritmo desde un checkpoint\n",
    "reload_checkpoint_dir = f\"/data/samuel_lozano/cooked/saved_models/map_{reload_map_nr}/{training_id}/Checkpoint_{checkpoint_nr}/\"\n",
    "\n",
    "reload_algo = Algorithm.from_checkpoint(reload_checkpoint_dir)\n",
    "\n",
    "policy_module_1 = reload_algo.get_policy(\"policy_ai_rl_1\")\n",
    "policy_module_2 = reload_algo.get_policy(\"policy_ai_rl_2\")\n",
    "\n",
    "# Comparar pesos (si estás usando PyTorch)\n",
    "params1 = policy_module_1.get_parameters()\n",
    "params2 = policy_module_2.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Deprecated.<locals>._inner.<locals>._ctor of PPO(env=spoiled_broth; env-runners=2; learners=0; multi-agent=True)>\n"
     ]
    }
   ],
   "source": [
    "params1 = dict(policy_module_1.model.named_parameters())\n",
    "params2 = dict(policy_module_2.model.named_parameters())\n",
    "\n",
    "for name in params1:\n",
    "    if name in params2:\n",
    "        are_equal = torch.allclose(params1[name], params2[name])\n",
    "        print(f\"{name}: {'Equal' if are_equal else 'Different'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a741846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_vector(params):\n",
    "    # Convierte diccionario de parámetros en un vector concatenado (PyTorch tensors)\n",
    "    vectors = []\n",
    "    for key, tensor in params.items():\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            vectors.append(tensor.flatten())\n",
    "        else:\n",
    "            # Si no es tensor, intenta convertirlo o ignora\n",
    "            pass\n",
    "    return torch.cat(vectors)\n",
    "\n",
    "v1 = params_to_vector(params1)\n",
    "v2 = params_to_vector(params2)\n",
    "\n",
    "diff = torch.norm(v1 - v2).item()\n",
    "print(f\"Diferencia entre parámetros de ambas políticas: {diff}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooked_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
