{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b5305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from spoiled_broth.rl.game_env import GameEnv\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import supersuit as ss\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model = 'PPO'\n",
    "map_nr = 'kitchen'\n",
    "n_iterations = 20\n",
    "save_every_n_iterations = 5\n",
    "w1_1 = float(1.0)\n",
    "w1_2 = float(0.0)\n",
    "w2_1 = float(0.7071)\n",
    "w2_2 = float(0.7071)\n",
    "\n",
    "# Define reward weights\n",
    "reward_weights = {\n",
    "    \"ai_rl_1\": (w1_1, w1_2),\n",
    "    \"ai_rl_2\": (w2_1, w2_2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6478af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "brigit = \"/mnt/lustre/home/samuloza\"\n",
    "base_save_dir = f\"{brigit}/data/samuel_lozano/cooked/saved_models/map_{map_nr}/\"\n",
    "Path(base_save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bbcd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(config):\n",
    "    # Your existing GameEnv class goes here\n",
    "    return GameEnv(**config)\n",
    "\n",
    "#def env_creator_2(config):\n",
    "#    env = GameEnv(reward_weights=reward_weights, map_nr=map_nr)\n",
    "#    env = ss.pad_observations_v0(env)\n",
    "#    env = ss.pad_action_space_v0(env)\n",
    "#    return PettingZooEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debe51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "#os.environ[\"RAY_TMPDIR\"] = f\"/data/samuel_lozano/tmp_ray/\"\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, #_temp_dir=os.environ[\"RAY_TMPDIR\"]\n",
    "         )\n",
    "\n",
    "# Register the environment with RLLib\n",
    "register_env(\"spoiled_broth\", lambda config: ParallelPettingZooEnv(env_creator(config)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f437163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify observation space consistency\n",
    "def verify_observation_space():\n",
    "    test_env = env_creator({\"reward_weights\": reward_weights, \"map_nr\": map_nr})\n",
    "    obs_space = test_env.observation_space(\"ai_rl_1\")\n",
    "    sample_obs = test_env.reset()[0][\"ai_rl_1\"]\n",
    "    \n",
    "    print(f\"Observation space: {obs_space}\")\n",
    "    print(f\"Sample observation shape: {sample_obs.shape}\")\n",
    "    print(f\"Sample observation min/max: {np.min(sample_obs)}, {np.max(sample_obs)}\")\n",
    "    \n",
    "    assert obs_space.contains(sample_obs), \"Observation doesn't match observation space!\"\n",
    "    test_env.close()\n",
    "    return obs_space\n",
    "\n",
    "# Get verified observation space\n",
    "obs_space = verify_observation_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bd9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate policies for each agent\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    return f\"policy_{agent_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d34d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureRewardFromInfoCallback(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, episode, **kwargs):\n",
    "        episode.user_data[\"pure_rewards\"] = defaultdict(float)\n",
    "\n",
    "    def on_postprocess_trajectory(self, *, episode, agent_id, policy_id, postprocessed_batch, original_batches, **kwargs):\n",
    "        _, infos = original_batches[agent_id]\n",
    "        for info in infos:\n",
    "            if \"pure_reward\" in info:\n",
    "                episode.user_data[\"pure_rewards\"][agent_id] += info[\"pure_reward\"]\n",
    "\n",
    "    def on_episode_end(self, *, episode, **kwargs):\n",
    "        for agent_id, pure_reward in episode.user_data[\"pure_rewards\"].items():\n",
    "            episode.custom_metrics[f\"pure_reward_{agent_id}\"] = pure_reward\n",
    "\n",
    "        episode.custom_metrics[\"pure_reward_mean\"] = sum(episode.user_data[\"pure_rewards\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for multi-agent training\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True\n",
    "    )\n",
    "    .environment(\n",
    "        env=\"spoiled_broth\",\n",
    "        env_config={\n",
    "            \"reward_weights\": reward_weights,\n",
    "            \"map_nr\": map_nr\n",
    "        },\n",
    "        clip_actions=True,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_ai_rl_1\": (\n",
    "                None,  # Use default PPO policy\n",
    "                env_creator({\"map_nr\": map_nr}).observation_space(\"ai_rl_1\"),\n",
    "                env_creator({\"map_nr\": map_nr}).action_space(\"ai_rl_1\"),\n",
    "                {}\n",
    "            ),\n",
    "            \"policy_ai_rl_2\": (\n",
    "                None,  # Use default PPO policy\n",
    "                env_creator({\"map_nr\": map_nr}).observation_space(\"ai_rl_2\"),\n",
    "                env_creator({\"map_nr\": map_nr}).action_space(\"ai_rl_2\"),\n",
    "                {}\n",
    "            )\n",
    "        },\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        policies_to_train=[\"policy_ai_rl_1\", \"policy_ai_rl_2\"]\n",
    "    )\n",
    "    #.resources(num_gpus=1)  # Set to 1 if you have a GPU\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        minibatch_size=500,\n",
    "        num_epochs=10\n",
    "    )\n",
    "    .callbacks(PureRewardFromInfoCallback)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370781c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build algorithm with error handling\n",
    "try:\n",
    "    algo = config.build_algo()\n",
    "    full_run_dir = algo.logdir\n",
    "    NAME_RAY = Path(full_run_dir).name\n",
    "    print(f\"Algorithm built successfully: {NAME_RAY}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error building algorithm: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b210f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training config\n",
    "training_dir = Path(f\"{base_save_dir}{NAME_RAY}\")\n",
    "training_dir.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copytree(full_run_dir, training_dir, dirs_exist_ok=True)\n",
    "\n",
    "def safe_config_dict(d):\n",
    "    def make_serializable(o):\n",
    "        try:\n",
    "            json.dumps(o)\n",
    "            return o\n",
    "        except TypeError:\n",
    "            return str(o)\n",
    "\n",
    "    return {\n",
    "        k: make_serializable(v)\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "\n",
    "config_path = f\"{training_dir}/config.txt\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(\"==== Training parameters ====\\n\")\n",
    "    f.write(f\"RL model: {rl_model}\\n\")\n",
    "    f.write(f\"Map id: {map_nr}\\n\")\n",
    "    f.write(f\"Number of iterations: {n_iterations}\\n\")\n",
    "    f.write(f\"Saved every N iterations: {save_every_n_iterations}\\n\")\n",
    "    f.write(f\"Reward weights ai_rl_1: {w1_1}, {w1_2}\\n\")\n",
    "    f.write(f\"Reward weights ai_rl_2: {w2_1}, {w2_2}\\n\")\n",
    "    f.write(\"\\n==== Complete configuration ====\\n\")\n",
    "    f.write(json.dumps(safe_config_dict(config.to_dict()), indent=4))\n",
    "\n",
    "csv_file_path = f'{training_dir}/reward_data.csv'\n",
    "with open(csv_file_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"iteration\", \"total_reward\", \"reward_agent_1\", \"reward_agent_2\", \"pure_reward_mean\", \"pure_reward_agent_1\", \"pure_reward_agent_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_iterations): \n",
    "    result = algo.train()\n",
    "    reward_agent_1 = result['env_runners']['module_episode_returns_mean'].get('policy_ai_rl_1', 0)\n",
    "    reward_agent_2 = result['env_runners']['module_episode_returns_mean'].get('policy_ai_rl_2', 0)\n",
    "    total_reward = result['env_runners']['episode_return_mean']\n",
    "    custom_metrics = result.get(\"custom_metrics\", {})\n",
    "    pure_reward_agent_1 = custom_metrics.get(\"pure_reward_agent_1\", 0.0)\n",
    "    pure_reward_agent_2 = custom_metrics.get(\"pure_reward_agent_2\", 0.0)\n",
    "    pure_reward_mean = custom_metrics.get(\"pure_reward_mean\", 0.0)\n",
    "\n",
    "    print(f\"Iteration {i}:\")\n",
    "    print(f\"  Agent 1 reward: {reward_agent_1}\")\n",
    "    print(f\"  Agent 2 reward: {reward_agent_2}\")\n",
    "    print(f\"  Total reward: {total_reward}\")\n",
    "    print(f\"  Pure reward agent 1: {pure_reward_agent_1}\")\n",
    "    print(f\"  Pure reward agent 2: {pure_reward_agent_2}\")\n",
    "    print(f\"  Total pure reward: {pure_reward_mean}\")\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i, total_reward, reward_agent_1, reward_agent_2, pure_reward_mean, pure_reward_agent_1, pure_reward_agent_2])\n",
    "\n",
    "    # Save checkpoint\n",
    "    if i % save_every_n_iterations == 0:\n",
    "        checkpoint_obtained = algo.save()\n",
    "        checkpoint_path = Path(checkpoint_obtained.checkpoint.path)\n",
    "        \n",
    "        custom_checkpoint_dir = Path(f\"{training_dir}/Checkpoint_{i}\")\n",
    "        custom_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for item in checkpoint_path.iterdir():\n",
    "            dst = custom_checkpoint_dir / item.name\n",
    "            if item.is_dir():\n",
    "                shutil.copytree(item, dst, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(item, dst)\n",
    "\n",
    "        print(f\"Checkpoint {i} saved in {custom_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64f064",
   "metadata": {},
   "source": [
    "# Reload params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1477dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b037174",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_map_nr = 1\n",
    "\n",
    "training_id = \"PPO_spoiled_broth_2025-05-18_22-22-47j6l54y_m\"\n",
    "\n",
    "checkpoint_nr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4aacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaura el algoritmo desde un checkpoint\n",
    "reload_checkpoint_dir = f\"/data/samuel_lozano/cooked/saved_models/map_{reload_map_nr}/{training_id}/Checkpoint_{checkpoint_nr}/\"\n",
    "\n",
    "reload_algo = Algorithm.from_checkpoint(reload_checkpoint_dir)\n",
    "\n",
    "policy_module_1 = reload_algo.get_policy(\"policy_ai_rl_1\")\n",
    "policy_module_2 = reload_algo.get_policy(\"policy_ai_rl_2\")\n",
    "\n",
    "# Comparar pesos (si estás usando PyTorch)\n",
    "params1 = policy_module_1.get_parameters()\n",
    "params2 = policy_module_2.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Deprecated.<locals>._inner.<locals>._ctor of PPO(env=spoiled_broth; env-runners=2; learners=0; multi-agent=True)>\n"
     ]
    }
   ],
   "source": [
    "params1 = dict(policy_module_1.model.named_parameters())\n",
    "params2 = dict(policy_module_2.model.named_parameters())\n",
    "\n",
    "for name in params1:\n",
    "    if name in params2:\n",
    "        are_equal = torch.allclose(params1[name], params2[name])\n",
    "        print(f\"{name}: {'Equal' if are_equal else 'Different'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a741846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_vector(params):\n",
    "    # Convierte diccionario de parámetros en un vector concatenado (PyTorch tensors)\n",
    "    vectors = []\n",
    "    for key, tensor in params.items():\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            vectors.append(tensor.flatten())\n",
    "        else:\n",
    "            # Si no es tensor, intenta convertirlo o ignora\n",
    "            pass\n",
    "    return torch.cat(vectors)\n",
    "\n",
    "v1 = params_to_vector(params1)\n",
    "v2 = params_to_vector(params2)\n",
    "\n",
    "diff = torch.norm(v1 - v2).item()\n",
    "print(f\"Diferencia entre parámetros de ambas políticas: {diff}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooked_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
