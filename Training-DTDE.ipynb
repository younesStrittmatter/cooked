{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b5305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from spoiled_broth.rl.game_env import GameEnv\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import supersuit as ss\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model = 'PPO'\n",
    "map_nr = 'kitchen'\n",
    "n_iterations = 20\n",
    "save_every_n_iterations = 5\n",
    "w1_1 = float(1.0)\n",
    "w1_2 = float(0.0)\n",
    "w2_1 = float(0.7071)\n",
    "w2_2 = float(0.7071)\n",
    "\n",
    "# Define reward weights\n",
    "reward_weights = {\n",
    "    \"ai_rl_1\": (w1_1, w1_2),\n",
    "    \"ai_rl_2\": (w2_1, w2_2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "brigit = \"/mnt/lustre/home/samuloza\"\n",
    "base_save_dir = f\"/data/samuel_lozano/cooked/saved_models/map_{map_nr}/\"\n",
    "Path(base_save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(config):\n",
    "    # Your existing GameEnv class goes here\n",
    "    return GameEnv(**config)\n",
    "\n",
    "#def env_creator_2(config):\n",
    "#    env = GameEnv(reward_weights=reward_weights, map_nr=map_nr)\n",
    "#    env = ss.pad_observations_v0(env)\n",
    "#    env = ss.pad_action_space_v0(env)\n",
    "#    return PettingZooEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4debe51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 16:41:16,336\tINFO worker.py:1888 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m Weighted rewards:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m Agent 1: (1.0, 0.0)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m Agent 2: (0.7071, 0.7071)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m 2025-05-21 16:42:42,364\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=2372605)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372605)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372605)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372605)\u001b[0m Weighted rewards:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372605)\u001b[0m Agent 2: (0.7071, 0.7071)\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=2372605)\u001b[0m 2025-05-21 16:42:59,679\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m 2025-05-21 16:43:04,829\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: 'MultiAgentEpisode' object has no attribute 'user_data'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m                      ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 222, in sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m         num_timesteps=num_timesteps,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     ...<2 lines>...\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m         force_reset=force_reset,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     )\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 277, in _sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 572, in _reset_envs\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     self._make_on_episode_callback(\"on_episode_start\", env_index, episodes)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 940, in _make_on_episode_callback\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     make_callback(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     ~~~~~~~~~~~~~^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m         which,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m         ^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     ...<2 lines>...\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m         kwargs=kwargs,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m         ^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     )\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     ^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/home/samuel_lozano/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/callbacks/utils.py\", line 32, in make_callback\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     getattr(callback_obj, callback_name)(*(args or ()), **(kwargs or {}))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m   File \"/tmp/ipykernel_2371825/3678392359.py\", line 3, in on_episode_start\n",
      "\u001b[36m(MultiAgentEnvRunner pid=2372624)\u001b[0m AttributeError: 'MultiAgentEpisode' object has no attribute 'user_data'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5c051ad98734854ff53d66f701000000 Worker ID: afb64deae584a4042e7bc904cd47e43e69f13dfd2a55105e99e17869 Node ID: 7345a0219eb83c8d67279cebec9eb09e2417fd520d2b277263d82e9f Worker IP address: 10.117.132.28 Worker port: 42277 Worker PID: 2372605 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "os.environ[\"RAY_TMPDIR\"] = f\"/data/samuel_lozano/tmp_ray/\"\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _temp_dir=os.environ[\"RAY_TMPDIR\"]\n",
    "         )\n",
    "\n",
    "# Register the environment with RLLib\n",
    "register_env(\"spoiled_broth\", lambda config: ParallelPettingZooEnv(env_creator(config)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f437163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (0.7071, 0.7071)\n",
      "\n",
      "Observation space: Box(0.0, 50.0, (1668,), float32)\n",
      "Sample observation shape: (1668,)\n",
      "Sample observation min/max: 0.0, 50.0\n"
     ]
    }
   ],
   "source": [
    "# Verify observation space consistency\n",
    "def verify_observation_space():\n",
    "    test_env = env_creator({\"reward_weights\": reward_weights, \"map_nr\": map_nr})\n",
    "    obs_space = test_env.observation_space(\"ai_rl_1\")\n",
    "    sample_obs = test_env.reset()[0][\"ai_rl_1\"]\n",
    "    \n",
    "    print(f\"Observation space: {obs_space}\")\n",
    "    print(f\"Sample observation shape: {sample_obs.shape}\")\n",
    "    print(f\"Sample observation min/max: {np.min(sample_obs)}, {np.max(sample_obs)}\")\n",
    "    \n",
    "    assert obs_space.contains(sample_obs), \"Observation doesn't match observation space!\"\n",
    "    test_env.close()\n",
    "    return obs_space\n",
    "\n",
    "# Get verified observation space\n",
    "obs_space = verify_observation_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bd9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate policies for each agent\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    return f\"policy_{agent_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d34d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureRewardFromInfoCallback(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, episode, **kwargs):\n",
    "        episode.user_data[\"pure_rewards\"] = defaultdict(float)\n",
    "\n",
    "    def on_postprocess_trajectory(self, *, episode, agent_id, policy_id, postprocessed_batch, original_batches, **kwargs):\n",
    "        _, infos = original_batches[agent_id]\n",
    "        for info in infos:\n",
    "            if \"pure_reward\" in info:\n",
    "                episode.user_data[\"pure_rewards\"][agent_id] += info[\"pure_reward\"]\n",
    "\n",
    "    def on_episode_end(self, *, episode, **kwargs):\n",
    "        for agent_id, pure_reward in episode.user_data[\"pure_rewards\"].items():\n",
    "            episode.custom_metrics[f\"pure_reward_{agent_id}\"] = pure_reward\n",
    "\n",
    "        episode.custom_metrics[\"pure_reward_mean\"] = sum(episode.user_data[\"pure_rewards\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration for multi-agent training\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True\n",
    "    )\n",
    "    .environment(\n",
    "        env=\"spoiled_broth\",\n",
    "        env_config={\n",
    "            \"reward_weights\": reward_weights,\n",
    "            \"map_nr\": map_nr\n",
    "        },\n",
    "        clip_actions=True,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_ai_rl_1\": (\n",
    "                None,  # Use default PPO policy\n",
    "                env_creator({\"map_nr\": map_nr}).observation_space(\"ai_rl_1\"),\n",
    "                env_creator({\"map_nr\": map_nr}).action_space(\"ai_rl_1\"),\n",
    "                {}\n",
    "            ),\n",
    "            \"policy_ai_rl_2\": (\n",
    "                None,  # Use default PPO policy\n",
    "                env_creator({\"map_nr\": map_nr}).observation_space(\"ai_rl_2\"),\n",
    "                env_creator({\"map_nr\": map_nr}).action_space(\"ai_rl_2\"),\n",
    "                {}\n",
    "            )\n",
    "        },\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        policies_to_train=[\"policy_ai_rl_1\", \"policy_ai_rl_2\"]\n",
    "    )\n",
    "    #.resources(num_gpus=1)  # Set to 1 if you have a GPU\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        minibatch_size=500,\n",
    "        num_epochs=10\n",
    "    )\n",
    "    .callbacks(PureRewardFromInfoCallback)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "370781c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 16:41:27,649\tWARNING algorithm_config.py:4968 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-05-21 16:43:01,504\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-05-21 16:43:04,420\tINFO trainable.py:160 -- Trainable.setup took 96.705 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm built successfully: PPO_spoiled_broth_2025-05-21_16-41-27_v692f15\n"
     ]
    }
   ],
   "source": [
    "# Build algorithm with error handling\n",
    "try:\n",
    "    algo = config.build_algo()\n",
    "    full_run_dir = algo.logdir\n",
    "    NAME_RAY = Path(full_run_dir).name\n",
    "    print(f\"Algorithm built successfully: {NAME_RAY}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error building algorithm: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b210f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training config\n",
    "training_dir = Path(f\"{base_save_dir}{NAME_RAY}\")\n",
    "training_dir.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copytree(full_run_dir, training_dir, dirs_exist_ok=True)\n",
    "\n",
    "def safe_config_dict(d):\n",
    "    def make_serializable(o):\n",
    "        try:\n",
    "            json.dumps(o)\n",
    "            return o\n",
    "        except TypeError:\n",
    "            return str(o)\n",
    "\n",
    "    return {\n",
    "        k: make_serializable(v)\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "\n",
    "config_path = f\"{training_dir}/config.txt\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(\"==== Training parameters ====\\n\")\n",
    "    f.write(f\"RL model: {rl_model}\\n\")\n",
    "    f.write(f\"Map id: {map_nr}\\n\")\n",
    "    f.write(f\"Number of iterations: {n_iterations}\\n\")\n",
    "    f.write(f\"Saved every N iterations: {save_every_n_iterations}\\n\")\n",
    "    f.write(f\"Reward weights ai_rl_1: {w1_1}, {w1_2}\\n\")\n",
    "    f.write(f\"Reward weights ai_rl_2: {w2_1}, {w2_2}\\n\")\n",
    "    f.write(\"\\n==== Complete configuration ====\\n\")\n",
    "    f.write(json.dumps(safe_config_dict(config.to_dict()), indent=4))\n",
    "\n",
    "csv_file_path = f'{training_dir}/reward_data.csv'\n",
    "with open(csv_file_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"iteration\", \"total_reward\", \"reward_agent_1\", \"reward_agent_2\", \"pure_reward_mean\", \"pure_reward_agent_1\", \"pure_reward_agent_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2da6567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 16:44:04,620\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2025-05-21 16:44:07,570\tERROR actor_manager.py:873 -- Ray error (The actor eeddc41a20a265ebc9effc9e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "2025-05-21 16:44:07,571\tERROR actor_manager.py:873 -- Ray error (The actor 5c051ad98734854ff53d66f701000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "2025-05-21 16:44:07,572\tERROR actor_manager.py:674 -- The actor eeddc41a20a265ebc9effc9e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2025-05-21 16:44:07,573\tERROR actor_manager.py:674 -- The actor 5c051ad98734854ff53d66f701000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'env_runners'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations): \n\u001b[32m      2\u001b[39m     result = algo.train()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     reward_agent_1 = \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menv_runners\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mmodule_episode_returns_mean\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mpolicy_ai_rl_1\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m     reward_agent_2 = result[\u001b[33m'\u001b[39m\u001b[33menv_runners\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmodule_episode_returns_mean\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mpolicy_ai_rl_2\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m      5\u001b[39m     total_reward = result[\u001b[33m'\u001b[39m\u001b[33menv_runners\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mepisode_return_mean\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'env_runners'"
     ]
    }
   ],
   "source": [
    "for i in range(n_iterations): \n",
    "    result = algo.train()\n",
    "    reward_agent_1 = result['env_runners']['module_episode_returns_mean'].get('policy_ai_rl_1', 0)\n",
    "    reward_agent_2 = result['env_runners']['module_episode_returns_mean'].get('policy_ai_rl_2', 0)\n",
    "    total_reward = result['env_runners']['episode_return_mean']\n",
    "    custom_metrics = result.get(\"custom_metrics\", {})\n",
    "    pure_reward_agent_1 = custom_metrics.get(\"pure_reward_agent_1\", 0.0)\n",
    "    pure_reward_agent_2 = custom_metrics.get(\"pure_reward_agent_2\", 0.0)\n",
    "    pure_reward_mean = custom_metrics.get(\"pure_reward_mean\", 0.0)\n",
    "\n",
    "    print(f\"Iteration {i}:\")\n",
    "    print(f\"  Agent 1 reward: {reward_agent_1}\")\n",
    "    print(f\"  Agent 2 reward: {reward_agent_2}\")\n",
    "    print(f\"  Total reward: {total_reward}\")\n",
    "    print(f\"  Pure reward agent 1: {pure_reward_agent_1}\")\n",
    "    print(f\"  Pure reward agent 2: {pure_reward_agent_2}\")\n",
    "    print(f\"  Total pure reward: {pure_reward_mean}\")\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i, total_reward, reward_agent_1, reward_agent_2, pure_reward_mean, pure_reward_agent_1, pure_reward_agent_2])\n",
    "\n",
    "    # Save checkpoint\n",
    "    if i % save_every_n_iterations == 0:\n",
    "        checkpoint_obtained = algo.save()\n",
    "        checkpoint_path = Path(checkpoint_obtained.checkpoint.path)\n",
    "        \n",
    "        custom_checkpoint_dir = Path(f\"{training_dir}/Checkpoint_{i}\")\n",
    "        custom_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for item in checkpoint_path.iterdir():\n",
    "            dst = custom_checkpoint_dir / item.name\n",
    "            if item.is_dir():\n",
    "                shutil.copytree(item, dst, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(item, dst)\n",
    "\n",
    "        print(f\"Checkpoint {i} saved in {custom_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64f064",
   "metadata": {},
   "source": [
    "# Reload params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1477dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b037174",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_map_nr = 1\n",
    "\n",
    "training_id = \"PPO_spoiled_broth_2025-05-18_22-22-47j6l54y_m\"\n",
    "\n",
    "checkpoint_nr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4aacc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:48:33,277\tINFO trainable.py:160 -- Trainable.setup took 14.885 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgentEnvRunner' object has no attribute 'get_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m reload_checkpoint_dir = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/data/samuel_lozano/cooked/saved_models/map_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreload_map_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/Checkpoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m reload_algo = Algorithm.from_checkpoint(reload_checkpoint_dir)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m policy_module_1 = \u001b[43mreload_algo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpolicy_ai_rl_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m policy_module_2 = reload_algo.get_policy(\u001b[33m\"\u001b[39m\u001b[33mpolicy_ai_rl_2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Comparar pesos (si estás usando PyTorch)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/algorithms/algorithm.py:2389\u001b[39m, in \u001b[36mAlgorithm.get_policy\u001b[39m\u001b[34m(self, policy_id)\u001b[39m\n\u001b[32m   2382\u001b[39m \u001b[38;5;129m@OldAPIStack\u001b[39m\n\u001b[32m   2383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_id: PolicyID = DEFAULT_POLICY_ID) -> Policy:\n\u001b[32m   2384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return policy for the specified id, or None.\u001b[39;00m\n\u001b[32m   2385\u001b[39m \n\u001b[32m   2386\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   2387\u001b[39m \u001b[33;03m        policy_id: ID of the policy to return.\u001b[39;00m\n\u001b[32m   2388\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m(policy_id)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MultiAgentEnvRunner' object has no attribute 'get_policy'"
     ]
    }
   ],
   "source": [
    "# Restaura el algoritmo desde un checkpoint\n",
    "reload_checkpoint_dir = f\"/data/samuel_lozano/cooked/saved_models/map_{reload_map_nr}/{training_id}/Checkpoint_{checkpoint_nr}/\"\n",
    "\n",
    "reload_algo = Algorithm.from_checkpoint(reload_checkpoint_dir)\n",
    "\n",
    "policy_module_1 = reload_algo.get_policy(\"policy_ai_rl_1\")\n",
    "policy_module_2 = reload_algo.get_policy(\"policy_ai_rl_2\")\n",
    "\n",
    "# Comparar pesos (si estás usando PyTorch)\n",
    "params1 = policy_module_1.get_parameters()\n",
    "params2 = policy_module_2.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Deprecated.<locals>._inner.<locals>._ctor of PPO(env=spoiled_broth; env-runners=2; learners=0; multi-agent=True)>\n"
     ]
    }
   ],
   "source": [
    "params1 = dict(policy_module_1.model.named_parameters())\n",
    "params2 = dict(policy_module_2.model.named_parameters())\n",
    "\n",
    "for name in params1:\n",
    "    if name in params2:\n",
    "        are_equal = torch.allclose(params1[name], params2[name])\n",
    "        print(f\"{name}: {'Equal' if are_equal else 'Different'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a741846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_vector(params):\n",
    "    # Convierte diccionario de parámetros en un vector concatenado (PyTorch tensors)\n",
    "    vectors = []\n",
    "    for key, tensor in params.items():\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            vectors.append(tensor.flatten())\n",
    "        else:\n",
    "            # Si no es tensor, intenta convertirlo o ignora\n",
    "            pass\n",
    "    return torch.cat(vectors)\n",
    "\n",
    "v1 = params_to_vector(params1)\n",
    "v2 = params_to_vector(params2)\n",
    "\n",
    "diff = torch.norm(v1 - v2).item()\n",
    "print(f\"Diferencia entre parámetros de ambas políticas: {diff}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooked",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
