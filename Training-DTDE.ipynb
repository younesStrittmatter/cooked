{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b5305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from spoiled_broth.rl.game_env import GameEnv\n",
    "from pathlib import Path\n",
    "import supersuit as ss\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model = 'PPO'\n",
    "map_nr = 1\n",
    "\n",
    "# Define reward weights\n",
    "reward_weights = {\n",
    "    \"ai_rl_1\": (0.5, 0.5),  # agent 1 is full cooperative\n",
    "    \"ai_rl_2\": (0.9, 0.1),  # agent 2 is less cooperative\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6478af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_save_dir = f\"/data/samuel_lozano/cooked/saved_models/map_{map_nr}/\"\n",
    "Path(base_save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bbcd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(config):\n",
    "    # Your existing GameEnv class goes here\n",
    "    return GameEnv(**config)\n",
    "\n",
    "#def env_creator_2(config):\n",
    "#    env = GameEnv(reward_weights=reward_weights, map_nr=map_nr)\n",
    "#    env = ss.pad_observations_v0(env)\n",
    "#    env = ss.pad_action_space_v0(env)\n",
    "#    return PettingZooEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4debe51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 23:05:16,592\tINFO worker.py:1888 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Weighted rewards:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent 1: (0.5, 0.5)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent 2: (0.9, 0.1)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m 2025-05-18 23:05:28,960\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 3\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 3\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 3\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 3\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 3\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 4\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 5\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 3\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 2\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 3\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607652)\u001b[0m Agent delivered item, new score: 1\n",
      "\u001b[36m(MultiAgentEnvRunner pid=1607667)\u001b[0m Agent delivered item, new score: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "os.environ[\"RAY_TMPDIR\"] = \"/data/samuel_lozano/tmp_ray/\"\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, _temp_dir=os.environ[\"RAY_TMPDIR\"])\n",
    "\n",
    "# Register the environment with RLLib\n",
    "register_env(\"spoiled_broth\", lambda config: ParallelPettingZooEnv(env_creator(config)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f437163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (0.5, 0.5)\n",
      "\n",
      "Agent 2: (0.9, 0.1)\n",
      "\n",
      "Observation space: Box(0.0, 50.0, (1668,), float32)\n",
      "Sample observation shape: (1668,)\n",
      "Sample observation min/max: 0.0, 50.0\n"
     ]
    }
   ],
   "source": [
    "# Verify observation space consistency\n",
    "def verify_observation_space():\n",
    "    test_env = env_creator({\"reward_weights\": reward_weights, \"map_nr\": 1})\n",
    "    obs_space = test_env.observation_space(\"ai_rl_1\")\n",
    "    sample_obs = test_env.reset()[0][\"ai_rl_1\"]\n",
    "    \n",
    "    print(f\"Observation space: {obs_space}\")\n",
    "    print(f\"Sample observation shape: {sample_obs.shape}\")\n",
    "    print(f\"Sample observation min/max: {np.min(sample_obs)}, {np.max(sample_obs)}\")\n",
    "    \n",
    "    assert obs_space.contains(sample_obs), \"Observation doesn't match observation space!\"\n",
    "    test_env.close()\n",
    "    return obs_space\n",
    "\n",
    "# Get verified observation space\n",
    "obs_space = verify_observation_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bd9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate policies for each agent\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    return f\"policy_{agent_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a6d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n",
      "Weighted rewards:\n",
      "\n",
      "Agent 1: (1.0, 0.0)\n",
      "\n",
      "Agent 2: (1.0, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration for multi-agent training\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True\n",
    "    )\n",
    "    .environment(\n",
    "        env=\"spoiled_broth\",\n",
    "        env_config={\n",
    "            \"reward_weights\": reward_weights,\n",
    "            \"map_nr\": map_nr\n",
    "        },\n",
    "        clip_actions=True,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_ai_rl_1\": (\n",
    "                None,  # Use default PPO policy\n",
    "                env_creator({\"map_nr\": 1}).observation_space(\"ai_rl_1\"),\n",
    "                env_creator({\"map_nr\": 1}).action_space(\"ai_rl_1\"),\n",
    "                {}\n",
    "            ),\n",
    "            \"policy_ai_rl_2\": (\n",
    "                None,  # Use default PPO policy\n",
    "                env_creator({\"map_nr\": 1}).observation_space(\"ai_rl_2\"),\n",
    "                env_creator({\"map_nr\": 1}).action_space(\"ai_rl_2\"),\n",
    "                {}\n",
    "            )\n",
    "        },\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        policies_to_train=[\"policy_ai_rl_1\", \"policy_ai_rl_2\"]\n",
    "    )\n",
    "    .resources(num_gpus=1)  # Set to 1 if you have a GPU\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        minibatch_size=500,\n",
    "        num_epochs=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "370781c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 23:05:18,998\tWARNING algorithm_config.py:4968 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 23:05:31,665\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-05-18 23:05:34,294\tINFO trainable.py:160 -- Trainable.setup took 15.265 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm built successfully: PPO_spoiled_broth_2025-05-18_23-05-196iy07m5r\n"
     ]
    }
   ],
   "source": [
    "# Build algorithm with error handling\n",
    "try:\n",
    "    algo = config.build_algo()\n",
    "    full_run_dir = algo.logdir\n",
    "    NAME_RAY = Path(full_run_dir).name\n",
    "    print(f\"Algorithm built successfully: {NAME_RAY}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error building algorithm: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b210f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n",
      "  Agent 1 reward: -36.35749999999997\n",
      "  Agent 2 reward: -36.007500000000164\n",
      "  Total reward: -72.36500000000014\n",
      "Checkpoint 0 guardado en /data/samuel_lozano/cooked/saved_models/map_1/PPO_spoiled_broth_2025-05-18_23-05-196iy07m5r/Checkpoint_0\n",
      "Iteration 1:\n",
      "  Agent 1 reward: -40.492500000000014\n",
      "  Agent 2 reward: -40.14583333333356\n",
      "  Total reward: -80.63833333333355\n",
      "Iteration 2:\n",
      "  Agent 1 reward: -40.973541666666684\n",
      "  Agent 2 reward: -40.151041666666885\n",
      "  Total reward: -81.12458333333358\n",
      "Iteration 3:\n",
      "  Agent 1 reward: -41.67862500000001\n",
      "  Agent 2 reward: -40.73412500000021\n",
      "  Total reward: -82.41275000000023\n",
      "Iteration 4:\n",
      "  Agent 1 reward: -42.7663\n",
      "  Agent 2 reward: -41.64270000000021\n",
      "  Total reward: -84.40900000000022\n",
      "Iteration 5:\n",
      "  Agent 1 reward: -42.86519999999999\n",
      "  Agent 2 reward: -41.463600000000184\n",
      "  Total reward: -84.32880000000017\n",
      "Checkpoint 5 guardado en /data/samuel_lozano/cooked/saved_models/map_1/PPO_spoiled_broth_2025-05-18_23-05-196iy07m5r/Checkpoint_5\n",
      "Iteration 6:\n",
      "  Agent 1 reward: -40.658099999999976\n",
      "  Agent 2 reward: -38.91290000000016\n",
      "  Total reward: -79.57100000000015\n",
      "Iteration 7:\n",
      "  Agent 1 reward: -39.61309999999997\n",
      "  Agent 2 reward: -37.603100000000154\n",
      "  Total reward: -77.21620000000011\n",
      "Iteration 8:\n",
      "  Agent 1 reward: -38.52309999999995\n",
      "  Agent 2 reward: -36.33310000000014\n",
      "  Total reward: -74.85620000000009\n",
      "Iteration 9:\n",
      "  Agent 1 reward: -38.232099999999946\n",
      "  Agent 2 reward: -36.23170000000012\n",
      "  Total reward: -74.46380000000006\n",
      "Iteration 10:\n",
      "  Agent 1 reward: -36.05359999999994\n",
      "  Agent 2 reward: -33.82640000000012\n",
      "  Total reward: -69.88000000000007\n",
      "Checkpoint 10 guardado en /data/samuel_lozano/cooked/saved_models/map_1/PPO_spoiled_broth_2025-05-18_23-05-196iy07m5r/Checkpoint_10\n",
      "Iteration 11:\n",
      "  Agent 1 reward: -32.46959999999994\n",
      "  Agent 2 reward: -29.40000000000011\n",
      "  Total reward: -61.869600000000055\n",
      "Iteration 12:\n",
      "  Agent 1 reward: -30.956999999999944\n",
      "  Agent 2 reward: -27.099400000000095\n",
      "  Total reward: -58.056400000000046\n",
      "Iteration 13:\n",
      "  Agent 1 reward: -29.681999999999935\n",
      "  Agent 2 reward: -25.518800000000084\n",
      "  Total reward: -55.200800000000015\n",
      "Iteration 14:\n",
      "  Agent 1 reward: -24.58649999999993\n",
      "  Agent 2 reward: -19.62850000000009\n",
      "  Total reward: -44.21500000000004\n",
      "Iteration 15:\n",
      "  Agent 1 reward: -19.22049999999993\n",
      "  Agent 2 reward: -13.144100000000108\n",
      "  Total reward: -32.36460000000004\n",
      "Checkpoint 15 guardado en /data/samuel_lozano/cooked/saved_models/map_1/PPO_spoiled_broth_2025-05-18_23-05-196iy07m5r/Checkpoint_15\n",
      "Iteration 16:\n",
      "  Agent 1 reward: -15.101499999999936\n",
      "  Agent 2 reward: -7.69390000000014\n",
      "  Total reward: -22.795400000000075\n",
      "Iteration 17:\n",
      "  Agent 1 reward: -10.460499999999945\n",
      "  Agent 2 reward: -1.4945000000001618\n",
      "  Total reward: -11.955000000000108\n",
      "Iteration 18:\n",
      "  Agent 1 reward: -8.899499999999932\n",
      "  Agent 2 reward: 0.6336999999998493\n",
      "  Total reward: -8.265800000000084\n",
      "Iteration 19:\n",
      "  Agent 1 reward: -8.073499999999914\n",
      "  Agent 2 reward: 2.036499999999847\n",
      "  Total reward: -6.037000000000068\n",
      "Iteration 20:\n",
      "  Agent 1 reward: 5.621500000000086\n",
      "  Agent 2 reward: 20.16189999999984\n",
      "  Total reward: 25.78339999999993\n",
      "Checkpoint 20 guardado en /data/samuel_lozano/cooked/saved_models/map_1/PPO_spoiled_broth_2025-05-18_23-05-196iy07m5r/Checkpoint_20\n",
      "Iteration 21:\n",
      "  Agent 1 reward: 6.848500000000049\n",
      "  Agent 2 reward: 23.3024999999998\n",
      "  Total reward: 30.15099999999985\n",
      "Iteration 22:\n",
      "  Agent 1 reward: 10.386000000000054\n",
      "  Agent 2 reward: 28.607599999999774\n",
      "  Total reward: 38.99359999999983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 01:20:37,376\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23:\n",
      "  Agent 1 reward: 10.386000000000054\n",
      "  Agent 2 reward: 28.607599999999774\n",
      "  Total reward: 38.99359999999983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 01:21:47,446\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24:\n",
      "  Agent 1 reward: 10.386000000000054\n",
      "  Agent 2 reward: 28.607599999999774\n",
      "  Total reward: 38.99359999999983\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     json.dump(safe_config_dict(config.to_dict()), f, indent=\u001b[32m4\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m30\u001b[39m): \n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     result = \u001b[43malgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Agent 1 reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33menv_runners\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmodule_episode_returns_mean\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mpolicy_ai_rl_1\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/tune/trainable/trainable.py:327\u001b[39m, in \u001b[36mTrainable.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    325\u001b[39m start = time.time()\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    329\u001b[39m     skipped = skip_exceptions(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/algorithms/algorithm.py:1038\u001b[39m, in \u001b[36mAlgorithm.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.enable_env_runner_and_connector_v2:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m         train_results, train_iter_ctx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1040\u001b[39m         (\n\u001b[32m   1041\u001b[39m             train_results,\n\u001b[32m   1042\u001b[39m             train_iter_ctx,\n\u001b[32m   1043\u001b[39m         ) = \u001b[38;5;28mself\u001b[39m._run_one_training_iteration_old_api_stack()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/algorithms/algorithm.py:3333\u001b[39m, in \u001b[36mAlgorithm._run_one_training_iteration\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3331\u001b[39m \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[32m   3332\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metrics.log_time((TIMERS, TRAINING_STEP_TIMER)):\n\u001b[32m-> \u001b[39m\u001b[32m3333\u001b[39m     training_step_return_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3334\u001b[39m     has_run_once = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3336\u001b[39m \u001b[38;5;66;03m# On the new API stack, results should NOT be returned anymore as\u001b[39;00m\n\u001b[32m   3337\u001b[39m \u001b[38;5;66;03m# a dict, but purely logged through the `MetricsLogger` API. This\u001b[39;00m\n\u001b[32m   3338\u001b[39m \u001b[38;5;66;03m# way, we make sure to never miss a single stats/counter/timer\u001b[39;00m\n\u001b[32m   3339\u001b[39m \u001b[38;5;66;03m# when calling `self.training_step()` more than once within the same\u001b[39;00m\n\u001b[32m   3340\u001b[39m \u001b[38;5;66;03m# iteration.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/algorithms/ppo/ppo.py:407\u001b[39m, in \u001b[36mPPO.training_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    397\u001b[39m     episodes, env_runner_results = synchronous_parallel_sample(\n\u001b[32m    398\u001b[39m         worker_set=\u001b[38;5;28mself\u001b[39m.env_runner_group,\n\u001b[32m    399\u001b[39m         max_agent_steps=\u001b[38;5;28mself\u001b[39m.config.total_train_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    404\u001b[39m         _return_metrics=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    405\u001b[39m     )\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     episodes, env_runner_results = \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_uses_new_env_runners\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_return_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# Return early if all our workers failed.\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/execution/rollout_ops.py:106\u001b[39m, in \u001b[36msynchronous_parallel_sample\u001b[39m\u001b[34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, random_actions, _uses_new_env_runners, _return_metrics)\u001b[39m\n\u001b[32m    103\u001b[39m         stats_dicts = [worker_set.local_env_runner.get_metrics()]\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     sampled_data = \u001b[43mworker_set\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforeach_env_runner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set.num_healthy_remote_workers() <= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/env/env_runner_group.py:843\u001b[39m, in \u001b[36mEnvRunnerGroup.foreach_env_runner\u001b[39m\u001b[34m(self, func, kwargs, local_env_runner, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._worker_manager.actor_ids():\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m remote_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_worker_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    853\u001b[39m FaultTolerantActorManager.handle_remote_call_result_errors(\n\u001b[32m    854\u001b[39m     remote_results, ignore_ray_errors=\u001b[38;5;28mself\u001b[39m._ignore_ray_errors_on_env_runners\n\u001b[32m    855\u001b[39m )\n\u001b[32m    857\u001b[39m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/utils/actor_manager.py:461\u001b[39m, in \u001b[36mFaultTolerantActorManager.foreach_actor\u001b[39m\u001b[34m(self, func, kwargs, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    454\u001b[39m remote_calls = \u001b[38;5;28mself\u001b[39m._call_actors(\n\u001b[32m    455\u001b[39m     func=func,\n\u001b[32m    456\u001b[39m     kwargs=kwargs,\n\u001b[32m    457\u001b[39m     remote_actor_ids=remote_actor_ids,\n\u001b[32m    458\u001b[39m )\n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m _, remote_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/utils/actor_manager.py:839\u001b[39m, in \u001b[36mFaultTolerantActorManager._fetch_result\u001b[39m\u001b[34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m readies, _ = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[32m    848\u001b[39m remote_results = RemoteCallResults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/_private/auto_init_hook.py:21\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     20\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/_private/client_mode_hook.py:103\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    102\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/_private/worker.py:3053\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[39m\n\u001b[32m   3051\u001b[39m timeout = timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m10\u001b[39m**\u001b[32m6\u001b[39m\n\u001b[32m   3052\u001b[39m timeout_milliseconds = \u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3053\u001b[39m ready_ids, remaining_ids = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/_raylet.pyx:3514\u001b[39m, in \u001b[36mray._raylet.CoreWorker.wait\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/includes/common.pxi:83\u001b[39m, in \u001b[36mray._raylet.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Save training config\n",
    "training_dir = Path(f\"{base_save_dir}{NAME_RAY}\")\n",
    "training_dir.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copytree(full_run_dir, training_dir, dirs_exist_ok=True)\n",
    "\n",
    "def safe_config_dict(d):\n",
    "    def make_serializable(o):\n",
    "        try:\n",
    "            json.dumps(o)\n",
    "            return o\n",
    "        except TypeError:\n",
    "            return str(o)\n",
    "\n",
    "    return {\n",
    "        k: make_serializable(v)\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "\n",
    "config_path = f\"{training_dir}/config.txt\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(safe_config_dict(config.to_dict()), f, indent=4)\n",
    "\n",
    "for i in range(30): \n",
    "    result = algo.train()\n",
    "    print(f\"Iteration {i}:\")\n",
    "    print(f\"  Agent 1 reward: {result['env_runners']['module_episode_returns_mean'].get('policy_ai_rl_1', 0)}\")\n",
    "    print(f\"  Agent 2 reward: {result['env_runners']['module_episode_returns_mean'].get('policy_ai_rl_2', 0)}\")\n",
    "    print(f\"  Total reward: {result['env_runners']['episode_return_mean']}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_obtained = algo.save()\n",
    "        checkpoint_path = Path(checkpoint_obtained.checkpoint.path)\n",
    "        \n",
    "        custom_checkpoint_dir = Path(f\"{training_dir}/Checkpoint_{i}\")\n",
    "        custom_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for item in checkpoint_path.iterdir():\n",
    "            dst = custom_checkpoint_dir / item.name\n",
    "            if item.is_dir():\n",
    "                shutil.copytree(item, dst, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(item, dst)\n",
    "\n",
    "        print(f\"Checkpoint {i} saved in {custom_checkpoint_dir}\")\n",
    "        \n",
    "        #eval_result = algo.evaluate()\n",
    "        #print(f\"Evaluation reward: {eval_result['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64f064",
   "metadata": {},
   "source": [
    "# Reload params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1477dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b037174",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_map_nr = 1\n",
    "\n",
    "training_id = \"PPO_spoiled_broth_2025-05-18_22-22-47j6l54y_m\"\n",
    "\n",
    "checkpoint_nr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf4aacc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:48:33,277\tINFO trainable.py:160 -- Trainable.setup took 14.885 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgentEnvRunner' object has no attribute 'get_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m reload_checkpoint_dir = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/data/samuel_lozano/cooked/saved_models/map_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreload_map_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/Checkpoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m reload_algo = Algorithm.from_checkpoint(reload_checkpoint_dir)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m policy_module_1 = \u001b[43mreload_algo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpolicy_ai_rl_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m policy_module_2 = reload_algo.get_policy(\u001b[33m\"\u001b[39m\u001b[33mpolicy_ai_rl_2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Comparar pesos (si estÃ¡s usando PyTorch)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cooked/lib/python3.13/site-packages/ray/rllib/algorithms/algorithm.py:2389\u001b[39m, in \u001b[36mAlgorithm.get_policy\u001b[39m\u001b[34m(self, policy_id)\u001b[39m\n\u001b[32m   2382\u001b[39m \u001b[38;5;129m@OldAPIStack\u001b[39m\n\u001b[32m   2383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_id: PolicyID = DEFAULT_POLICY_ID) -> Policy:\n\u001b[32m   2384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return policy for the specified id, or None.\u001b[39;00m\n\u001b[32m   2385\u001b[39m \n\u001b[32m   2386\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   2387\u001b[39m \u001b[33;03m        policy_id: ID of the policy to return.\u001b[39;00m\n\u001b[32m   2388\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m(policy_id)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MultiAgentEnvRunner' object has no attribute 'get_policy'"
     ]
    }
   ],
   "source": [
    "# Restaura el algoritmo desde un checkpoint\n",
    "reload_checkpoint_dir = f\"/data/samuel_lozano/cooked/saved_models/map_{reload_map_nr}/{training_id}/Checkpoint_{checkpoint_nr}/\"\n",
    "\n",
    "reload_algo = Algorithm.from_checkpoint(reload_checkpoint_dir)\n",
    "\n",
    "policy_module_1 = reload_algo.get_policy(\"policy_ai_rl_1\")\n",
    "policy_module_2 = reload_algo.get_policy(\"policy_ai_rl_2\")\n",
    "\n",
    "# Comparar pesos (si estÃ¡s usando PyTorch)\n",
    "params1 = policy_module_1.get_parameters()\n",
    "params2 = policy_module_2.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Deprecated.<locals>._inner.<locals>._ctor of PPO(env=spoiled_broth; env-runners=2; learners=0; multi-agent=True)>\n"
     ]
    }
   ],
   "source": [
    "params1 = dict(policy_module_1.model.named_parameters())\n",
    "params2 = dict(policy_module_2.model.named_parameters())\n",
    "\n",
    "for name in params1:\n",
    "    if name in params2:\n",
    "        are_equal = torch.allclose(params1[name], params2[name])\n",
    "        print(f\"{name}: {'Equal' if are_equal else 'Different'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a741846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_vector(params):\n",
    "    # Convierte diccionario de parÃ¡metros en un vector concatenado (PyTorch tensors)\n",
    "    vectors = []\n",
    "    for key, tensor in params.items():\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            vectors.append(tensor.flatten())\n",
    "        else:\n",
    "            # Si no es tensor, intenta convertirlo o ignora\n",
    "            pass\n",
    "    return torch.cat(vectors)\n",
    "\n",
    "v1 = params_to_vector(params1)\n",
    "v2 = params_to_vector(params2)\n",
    "\n",
    "diff = torch.norm(v1 - v2).item()\n",
    "print(f\"Diferencia entre parÃ¡metros de ambas polÃ­ticas: {diff}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooked",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
